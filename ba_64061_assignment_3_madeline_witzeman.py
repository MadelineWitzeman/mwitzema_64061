# -*- coding: utf-8 -*-
"""BA 64061 Assignment 3 Madeline Witzeman.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1FSGPG4kBaTg-x-PiCiTsntVFL5mXUaBf

# Assignment 3: Time-Series Data
# BA 64061-003
# Madeline Witzeman

# Loading the weather dataset
"""

!wget https://s3.amazonaws.com/keras-datasets/jena_climate_2009_2016.csv.zip
!unzip jena_climate_2009_2016.csv.zip

"""## Examining the data of the Jena weather dataset"""

import os
fname = os.path.join("jena_climate_2009_2016.csv")

with open(fname) as f:
    data = f.read()

lines = data.split("\n")
header = lines[0].split(",")
lines = lines[1:]
print(header)
print(len(lines))

"""By examining the dataset, I see that each record is a timestamp with 14 other variables tracking things like temperature (in degrees C), dewpoint, humidity, etc., and there are 420,451 records in total.

# Parsing the data
"""

import numpy as np
temperature = np.zeros((len(lines),))
raw_data = np.zeros((len(lines), len(header) - 1))
for i, line in enumerate(lines):
    values = [float(x) for x in line.split(",")[1:]]
    temperature[i] = values[1]
    raw_data[i, :] = values[:]

"""With the code above, the temperature data (variable to be predicted) is separated into its own array with all other data except for date/time in another array.

## Plotting the temperature timeseries
"""

from matplotlib import pyplot as plt
plt.plot(range(len(temperature)), temperature)

"""The data appears to span 8 years.

## Plotting the first 10 days of the temperature timeseries
"""

plt.plot(range(1440), temperature[:1440])

"""There are 144 data points per day (temperature recorded every 10 min each day). It becomes easier to identify a more daily pattern in the chart above.

## Establishing training (50%), validation (25%), and test (25%) datasets
"""

num_train_samples = int(0.5 * len(raw_data))
num_val_samples = int(0.25 * len(raw_data))
num_test_samples = len(raw_data) - num_train_samples - num_val_samples
print("num_train_samples:", num_train_samples)
print("num_val_samples:", num_val_samples)
print("num_test_samples:", num_test_samples)

"""# Preparing the data

## Normalizing the data
"""

mean = raw_data[:num_train_samples].mean(axis=0)
raw_data -= mean
std = raw_data[:num_train_samples].std(axis=0)
raw_data /= std

import numpy as np
from tensorflow import keras
int_sequence = np.arange(10)
dummy_dataset = keras.utils.timeseries_dataset_from_array(
    data=int_sequence[:-3],
    targets=int_sequence[3:],
    sequence_length=3,
    batch_size=2,
)

for inputs, targets in dummy_dataset:
    for i in range(inputs.shape[0]):
        print([int(x) for x in inputs[i]], int(targets[i]))

"""The dataset needs to be preprocessed into a format that can be utilized by a neural network. Since the numeric data is on different scales, it needs to be normalized.

## Instantiating datasets for training, validation, and testing
"""

sampling_rate = 6
sequence_length = 120
delay = sampling_rate * (sequence_length + 24 - 1)
batch_size = 256

train_dataset = keras.utils.timeseries_dataset_from_array(
    raw_data[:-delay],
    targets=temperature[delay:],
    sampling_rate=sampling_rate,
    sequence_length=sequence_length,
    shuffle=True,
    batch_size=batch_size,
    start_index=0,
    end_index=num_train_samples)

val_dataset = keras.utils.timeseries_dataset_from_array(
    raw_data[:-delay],
    targets=temperature[delay:],
    sampling_rate=sampling_rate,
    sequence_length=sequence_length,
    shuffle=True,
    batch_size=batch_size,
    start_index=num_train_samples,
    end_index=num_train_samples + num_val_samples)

test_dataset = keras.utils.timeseries_dataset_from_array(
    raw_data[:-delay],
    targets=temperature[delay:],
    sampling_rate=sampling_rate,
    sequence_length=sequence_length,
    shuffle=True,
    batch_size=batch_size,
    start_index=num_train_samples + num_val_samples)

"""## Inspecting the output of the training dataset"""

for samples, targets in train_dataset:
    print("samples shape:", samples.shape)
    print("targets shape:", targets.shape)
    break

"""# Machine Learning Problem: Predict the temperature 24 hours from a certain point in time

# "Model" #1: Establishing a "common sense" non-machine learning baseline
"""

def evaluate_naive_method(dataset):
    total_abs_err = 0.
    samples_seen = 0
    for samples, targets in dataset:
        preds = samples[:, -1, 1] * std[1] + mean[1]
        total_abs_err += np.sum(np.abs(preds - targets))
        samples_seen += samples.shape[0]
    return total_abs_err / samples_seen

print(f"Validation MAE: {evaluate_naive_method(val_dataset):.2f}")
print(f"Test MAE: {evaluate_naive_method(test_dataset):.2f}")

"""The "common sense" baseline I'll compare against during this analysis is the assumption that the temperature 24 hours from now is the same as the current temperature.

This produces a validation MAE of 2.44 deg C and test MAE of 2.62 deg C.

# Model #2: Recurrent Nueral Network (RNN) -- Basic Long Short Term Memory (LSTM) Approach
"""

from tensorflow import keras
from tensorflow.keras import layers

inputs = keras.Input(shape=(sequence_length, raw_data.shape[-1]))
x = layers.LSTM(16)(inputs)
outputs = layers.Dense(1)(x)
model = keras.Model(inputs, outputs)

callbacks = [
    keras.callbacks.ModelCheckpoint("jena_lstm.keras",
                                    save_best_only=True)
]
model.compile(optimizer="rmsprop", loss="mse", metrics=["mae"])
history = model.fit(train_dataset,
                    epochs=10,
                    validation_data=val_dataset,
                    callbacks=callbacks)

model = keras.models.load_model("jena_lstm.keras")
print(f"Test MAE: {model.evaluate(test_dataset)[1]:.2f}")

"""By utilizing a RNN (LSTN above), the validation MAE drops as low as ~2.38 deg C and the test MAE drops slightly to ~2.56 deg C.

It appears that using a RNN with the structure above slighty outperformed the common sense baseline.

In the next model, I'm going to employ dropout see if it improves model performance by combatting overfitting. I'm also doubling the layer units from 16 to 32.

# Model #3: Droupout-Regularized LSTM
"""

inputs = keras.Input(shape=(sequence_length, raw_data.shape[-1]))
x = layers.LSTM(32, recurrent_dropout=0.25)(inputs)
x = layers.Dropout(0.5)(x)
outputs = layers.Dense(1)(x)
model = keras.Model(inputs, outputs)

callbacks = [
    keras.callbacks.ModelCheckpoint("jena_lstm_dropout.keras",
                                    save_best_only=True)
]
model.compile(optimizer="rmsprop", loss="mse", metrics=["mae"])
history = model.fit(train_dataset,
                    epochs=50,
                    validation_data=val_dataset,
                    callbacks=callbacks)

model = keras.models.load_model("jena_lstm_dropout.keras")
print(f"Test MAE: {model.evaluate(test_dataset)[1]:.2f}")

"""Including dropout in the basic LSTM model hardly impacts performance: Validation MAE is ~2.36 deg C and Test MAE is 2.56 deg C. Although employing the dropout didn't significantly improve model performance, I'm still including it in future models as dropout typically combats overfitting.

If I had the computationtal power to continue utilizing 50 epochs during training I would. However, since the model performed the best within the first 5 epochs, I'm going to reduce the number of epochs back down to between 5 - 10 moving forward which saves significant computational time.

# Model #4: Dropout-Regularized Stacked LSTM (32 Units per Layer)
"""

inputs = keras.Input(shape=(sequence_length, raw_data.shape[-1]))
x = layers.LSTM(32, recurrent_dropout=0.5, return_sequences=True)(inputs)
x = layers.LSTM(32, recurrent_dropout=0.5)(x)
x = layers.Dropout(0.5)(x)
outputs = layers.Dense(1)(x)
model = keras.Model(inputs, outputs)

callbacks = [
    keras.callbacks.ModelCheckpoint("jena_stacked_lstm_dropout.keras",
                                    save_best_only=True)
]
model.compile(optimizer="rmsprop", loss="mse", metrics=["mae"])
history = model.fit(train_dataset,
                    epochs=10,
                    validation_data=val_dataset,
                    callbacks=callbacks)
model = keras.models.load_model("jena_stacked_lstm_dropout.keras")
print(f"Test MAE: {model.evaluate(test_dataset)[1]:.2f}")

"""Utilizing a dropout-regularized stacked LSTM with 32 units per layer approach produces a validation MAE of 2.45 deg C and a test MAE of 2.58 deg C. This is worse than the prior two approaches, and is also slightly worse than the common sense baseline approach on validation. This model is slightly better than the common sense baseline on test accuarcy. It appears that a stacked approach may decrease performance when using LSTM.

I'm going to double the units per layer to 64 in the next model to see if increasing the units per layer in a stacked model decreases performance.

# Model #5: Dropout-Regularized Stacked LSTM (64 Units per Layer)
"""

inputs = keras.Input(shape=(sequence_length, raw_data.shape[-1]))
x = layers.LSTM(64, recurrent_dropout=0.5, return_sequences=True)(inputs)
x = layers.LSTM(64, recurrent_dropout=0.5)(x)
x = layers.Dropout(0.5)(x)
outputs = layers.Dense(1)(x)
model = keras.Model(inputs, outputs)

callbacks = [
    keras.callbacks.ModelCheckpoint("jena_stacked_lstm_dropout_2.keras",
                                    save_best_only=True)
]
model.compile(optimizer="rmsprop", loss="mse", metrics=["mae"])
history = model.fit(train_dataset,
                    epochs=5,
                    validation_data=val_dataset,
                    callbacks=callbacks)
model = keras.models.load_model("jena_stacked_lstm_dropout_2.keras")
print(f"Test MAE: {model.evaluate(test_dataset)[1]:.2f}")

"""Using a dropout-regularized stacked LSTM with 64 units per layer produces a validation MAE of ~2.45 deg C and a test MAE of ~2.60 deg C. I've confirmed that increasing the layers per unit doesn't improve performance in a stacked LSTM approach, therefore, I will not use 64 units per layer when switching to the GRU appraoch.

# Model #6: Dropout-Regularized Stacked GRU (32 Units per Layer)
"""

inputs = keras.Input(shape=(sequence_length, raw_data.shape[-1]))
x = layers.GRU(32, recurrent_dropout=0.5, return_sequences=True)(inputs)
x = layers.GRU(32, recurrent_dropout=0.5)(x)
x = layers.Dropout(0.5)(x)
outputs = layers.Dense(1)(x)
model = keras.Model(inputs, outputs)

callbacks = [
    keras.callbacks.ModelCheckpoint("jena_stacked_gru_dropout.keras",
                                    save_best_only=True)
]
model.compile(optimizer="rmsprop", loss="mse", metrics=["mae"])
history = model.fit(train_dataset,
                    epochs=5,
                    validation_data=val_dataset,
                    callbacks=callbacks)
model = keras.models.load_model("jena_stacked_gru_dropout.keras")
print(f"Test MAE: {model.evaluate(test_dataset)[1]:.2f}")

"""Using GRU (instead of LSTM) with a dropout-regularized approach and 32 units per layer improved performance: validation MAE is 2.31 deg C and test MAE is 2.44 deg C.

This is the best performing model so far.

# Model #7: 1d_convnets + LSTM
"""

inputs = keras.Input(shape=(sequence_length, raw_data.shape[-1]))
x = layers.Conv1D(8, 24, activation="relu")(inputs)
x = layers.MaxPooling1D(2)(x)
x = layers.Conv1D(8, 12, activation="relu")(x)
x = layers.MaxPooling1D(2)(x)
x = layers.Conv1D(8, 6, activation="relu")(x)
x = layers.GlobalAveragePooling1D()(x)
x = layers.LSTM(16)(inputs)
outputs = layers.Dense(1)(x)
model = keras.Model(inputs, outputs)

callbacks = [
    keras.callbacks.ModelCheckpoint("jena_conv.keras",
                                    save_best_only=True)
]
model.compile(optimizer="rmsprop", loss="mse", metrics=["mae"])
history = model.fit(train_dataset,
                    epochs=10,
                    validation_data=val_dataset,
                    callbacks=callbacks)

model = keras.models.load_model("jena_conv.keras")
print(f"Test MAE: {model.evaluate(test_dataset)[1]:.2f}")

"""Including 1d_convnets with LSTM didn't improve model performance: validation MAE was 2.43 deg C and test MAE was 2.63 deg C.

# Conclusion
"""

import pandas

data_models = [['N/A', 'N/A', 'N/A', 'N/A', '2.44', '2.62'], ['LSTM', 'Unstacked', 'No', '16', '2.38', '2.56'], ['LSTM', 'Unstacked', 'Yes', '32', '2.36', '2.56'], ['LSTM', 'Stacked', 'Yes', '32', '2.45', '2.58'], ['LSTM', 'Stacked', 'Yes', '64', '2.45', '2.60'], ['GRU', 'Stacked', 'Yes', '32', '2.31', '2.44'], ['1d Convnet + LSTM', 'Stacked', 'No', '16', '2.43', '2.63']]

df = pandas.DataFrame(data_models, index=[1,2,3,4,5,6,7], columns=['Type of RNN', 'Stacked/Unstacked', 'Dropout Used?', 'Units per Layer', 'Val MAE (Deg C)', 'Test MAE (Deg C)'])
print(df)

"""In conclusion, when comparing against the "common sense model" baseline assumption, Model #6 (Dropout-Regularized Stacked GRU, 32 units per layer) performed the best with a validation MAE of 2.31 deg C and a test MAE of 2.44 deg C.

Utilizing dropout didn't appear to significantly improve model performance. Additionally, using a stacked approach and increasing the number of units per layer decreased model performance with LSTM. Switching to GRU when using the dropout-regularized stacked approach notably improved model performance. Lastly, using 1d convnets with a RNN (LSTM) approach hurt model performance.

Overall, it appears to be difficult to significantly outperform the common sense model when predicting weather.
"""