# -*- coding: utf-8 -*-
"""BA 64061 Assignment 4 Madeline Witzeman.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1MhuLg5zspdaI1OvmdOrO4wjd5E0avQrg

# Assignment 4: Text and Sequence Data
# BA 64061-003
# Madeline Witzeman

# Loading and Preparing the IMDb dataset
"""

!curl -O https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz
!tar -xf aclImdb_v1.tar.gz

!rm -r aclImdb/train/unsup

"""Inspecting some of the data prior to modeling:"""

!cat aclImdb/train/pos/4077_10.txt

"""## Based on the assignment instructions, I am validating on 10,000 samples (#3) and training on 100 samples (#2):"""

import os, pathlib, shutil, random

base_dir = pathlib.Path("aclImdb")
val_dir = base_dir / "val"
train_dir = base_dir / "train"
train_dir_2 = base_dir / "train2"
for category in ("neg", "pos"):
    os.makedirs(val_dir / category)
    files = os.listdir(train_dir / category)
    random.Random(1337).shuffle(files)
    num_val_samples = int(0.4 * len(files))
    val_files = files[-num_val_samples:]
    for fname in val_files:
        shutil.move(train_dir / category / fname,
                    val_dir / category / fname)

for category in ("neg", "pos"):
    os.makedirs(train_dir_2 / category)
    files = os.listdir(train_dir / category)
    random.Random(1337).shuffle(files)
    num_train_samples = int(0.0067 * len(files))
    train_files = files[-num_train_samples:]
    for fname in train_files:
        shutil.move(train_dir / category / fname,
                    train_dir_2 / category / fname)

from tensorflow import keras
batch_size = 32

train_ds = keras.utils.text_dataset_from_directory(
    "aclImdb/train", batch_size=batch_size
)
val_ds = keras.utils.text_dataset_from_directory(
    "aclImdb/val", batch_size=batch_size
)
test_ds = keras.utils.text_dataset_from_directory(
    "aclImdb/test", batch_size=batch_size
)
train_ds_2 = keras.utils.text_dataset_from_directory(
    "aclImdb/train2", batch_size=batch_size
)

"""## Displaying the shapes and data types of the first batch"""

for inputs, targets in train_ds_2:
    print("inputs.shape:", inputs.shape)
    print("inputs.dtype:", inputs.dtype)
    print("targets.shape:", targets.shape)
    print("targets.dtype:", targets.dtype)
    print("inputs[0]:", inputs[0])
    print("targets[0]:", targets[0])
    break

"""# Using the bag-of-words with multi-hot binary vectors approach to establish a baseline

## Preprocessing the datasets with a TextVectorization layer. Restricting the vocabulary to the top 10,000 words based on assignment instructions (#4):
"""

from tensorflow.keras.layers import TextVectorization
text_vectorization = TextVectorization(
    output_mode="int",
)

text_vectorization = TextVectorization(
    max_tokens=10000,
    output_mode="multi_hot",
)
text_only_train_ds = train_ds.map(lambda x, y: x)
text_vectorization.adapt(text_only_train_ds)

binary_1gram_train_ds = train_ds.map(
    lambda x, y: (text_vectorization(x), y),
    num_parallel_calls=4)
binary_1gram_val_ds = val_ds.map(
    lambda x, y: (text_vectorization(x), y),
    num_parallel_calls=4)
binary_1gram_test_ds = test_ds.map(
    lambda x, y: (text_vectorization(x), y),
    num_parallel_calls=4)
binary_1gram_train_ds2 = train_ds_2.map(
    lambda x, y: (text_vectorization(x), y),
    num_parallel_calls=4)

"""## Inspecting the output of the binary unigram dataset"""

for inputs, targets in binary_1gram_train_ds2:
    print("inputs.shape:", inputs.shape)
    print("inputs.dtype:", inputs.dtype)
    print("targets.shape:", targets.shape)
    print("targets.dtype:", targets.dtype)
    print("inputs[0]:", inputs[0])
    print("targets[0]:", targets[0])
    break

"""## The model-building utility"""

from tensorflow import keras
from tensorflow.keras import layers

def get_model(max_tokens=10000, hidden_dim=16):
    inputs = keras.Input(shape=(max_tokens,))
    x = layers.Dense(hidden_dim, activation="relu")(inputs)
    x = layers.Dropout(0.5)(x)
    outputs = layers.Dense(1, activation="sigmoid")(x)
    model = keras.Model(inputs, outputs)
    model.compile(optimizer="rmsprop",
                  loss="binary_crossentropy",
                  metrics=["accuracy"])
    return model

"""## Training and testing the binary unigram model"""

model = get_model()
model.summary()
callbacks = [
    keras.callbacks.ModelCheckpoint("binary_1gram.keras",
                                    save_best_only=True)
]
model.fit(binary_1gram_train_ds2.cache(),
          validation_data=binary_1gram_val_ds.cache(),
          epochs=10,
          callbacks=callbacks)
model = keras.models.load_model("binary_1gram.keras")
print(f"Test acc: {model.evaluate(binary_1gram_test_ds)[1]:.3f}")

"""By only using 100 training samples, this approach yields a validation accuracy of ~71.2% (at best) and a test accuracy of ~70.9%.

# Establishing a second model using an embedding layer

## Preparing integer sequence datasets. Based on the assignment instructions, I'm still limiting the vocabulary to the top 10,000 words (#4) and now cutting off the reviews after 150 words (#1):
"""

from tensorflow.keras import layers

max_length = 150
max_tokens = 10000
text_vectorization = layers.TextVectorization(
    max_tokens=max_tokens,
    output_mode="int",
    output_sequence_length=max_length,
)
text_vectorization.adapt(text_only_train_ds)

int_train_ds = train_ds.map(
    lambda x, y: (text_vectorization(x), y),
    num_parallel_calls=4)
int_val_ds = val_ds.map(
    lambda x, y: (text_vectorization(x), y),
    num_parallel_calls=4)
int_test_ds = test_ds.map(
    lambda x, y: (text_vectorization(x), y),
    num_parallel_calls=4)
int_train_ds_2 = train_ds_2.map(
    lambda x, y: (text_vectorization(x), y),
    num_parallel_calls=4)

"""## Building a sequence model with an embedding layer trained from scratch with masking enabled"""

embedding_layer = layers.Embedding(input_dim=max_tokens, output_dim=256)

import tensorflow as tf
inputs = keras.Input(shape=(None,), dtype="int64")
embedded = layers.Embedding(
    input_dim=max_tokens, output_dim=256, mask_zero=True)(inputs)
x = layers.Bidirectional(layers.LSTM(32))(embedded)
x = layers.Dropout(0.5)(x)
outputs = layers.Dense(1, activation="sigmoid")(x)
model = keras.Model(inputs, outputs)
model.compile(optimizer="rmsprop",
              loss="binary_crossentropy",
              metrics=["accuracy"])
model.summary()

callbacks = [
    keras.callbacks.ModelCheckpoint("embeddings_bidir_gru_with_masking.keras",
                                    save_best_only=True)
]
model.fit(int_train_ds_2, validation_data=int_val_ds, epochs=10, callbacks=callbacks)
model = keras.models.load_model("embeddings_bidir_gru_with_masking.keras")
print(f"Test acc: {model.evaluate(int_test_ds)[1]:.3f}")

"""I enabled masking with the embedded layer since it improved model performance in the textbook. This approach yields a validation accuracy of ~53.0% (at best) and a test accuracy of ~53.7%, which is worse than the baseline binary unigram model. I'm now going to see how it compares to the pretrained word embedding model training on only 100 samples.

# Establishing a third model using pretrained word embedding
"""

!wget http://nlp.stanford.edu/data/glove.6B.zip
!unzip -q glove.6B.zip

"""## Parsing the GloVe word-embeddings file"""

import numpy as np
path_to_glove_file = "glove.6B.100d.txt"

embeddings_index = {}
with open(path_to_glove_file) as f:
    for line in f:
        word, coefs = line.split(maxsplit=1)
        coefs = np.fromstring(coefs, "f", sep=" ")
        embeddings_index[word] = coefs

print(f"Found {len(embeddings_index)} word vectors.")

"""## Preparing the GloVe word-embeddings matrix"""

embedding_dim = 100

vocabulary = text_vectorization.get_vocabulary()
word_index = dict(zip(vocabulary, range(len(vocabulary))))

embedding_matrix = np.zeros((max_tokens, embedding_dim))
for word, i in word_index.items():
    if i < max_tokens:
        embedding_vector = embeddings_index.get(word)
    if embedding_vector is not None:
        embedding_matrix[i] = embedding_vector

embedding_layer = layers.Embedding(
    max_tokens,
    embedding_dim,
    embeddings_initializer=keras.initializers.Constant(embedding_matrix),
    trainable=False,
    mask_zero=True,
)

"""## Building a model that uses a pretrained Embedding layer"""

inputs = keras.Input(shape=(None,), dtype="int64")
embedded = embedding_layer(inputs)
x = layers.Bidirectional(layers.LSTM(32))(embedded)
x = layers.Dropout(0.5)(x)
outputs = layers.Dense(1, activation="sigmoid")(x)
model = keras.Model(inputs, outputs)
model.compile(optimizer="rmsprop",
              loss="binary_crossentropy",
              metrics=["accuracy"])
model.summary()

callbacks = [
    keras.callbacks.ModelCheckpoint("glove_embeddings_sequence_model.keras",
                                    save_best_only=True)
]
model.fit(int_train_ds_2, validation_data=int_val_ds, epochs=10, callbacks=callbacks)
model = keras.models.load_model("glove_embeddings_sequence_model.keras")
print(f"Test acc: {model.evaluate(int_test_ds)[1]:.3f}")

"""This approach yields a validation accuracy of ~54.0% (at best) and a test accuracy of ~54.7%, which is also worse than the baseline binary unigram model performance. This model performed very comparably to the model using an embedding layer trained from scratch. At times, the model with a pretrained word embedding layer performed better than the model with an embedding layer trained from scratch, and vice versa.

This makes sense given I am only training on a sample size of 100 movie reviews. When there isn't a large enough training sample to learn from the dataset itself, pretrained embedding layers can help improve model performance.

I'm going to alter the training sample size to see at what point the model utilizing the embedding layer trained from scracth notably outperforms the model utilizing the pretrained embedding layer.

# Increasing the training samples to 998 (almost 1,000)
"""

import os, pathlib, shutil, random

base_dir = pathlib.Path("aclImdb")
val_dir = base_dir / "val"
train_dir = base_dir / "train"
train_dir_3 = base_dir / "train3"

for category in ("neg", "pos"):
    os.makedirs(train_dir_3 / category)
    files = os.listdir(train_dir / category)
    random.Random(1337).shuffle(files)
    num_train_samples_2 = int(0.067 * len(files))
    train_files_2 = files[-num_train_samples_2:]
    for fname in train_files_2:
        shutil.move(train_dir / category / fname,
                    train_dir_3 / category / fname)

from tensorflow import keras
batch_size = 32

train_ds = keras.utils.text_dataset_from_directory(
    "aclImdb/train", batch_size=batch_size
)
val_ds = keras.utils.text_dataset_from_directory(
    "aclImdb/val", batch_size=batch_size
)
test_ds = keras.utils.text_dataset_from_directory(
    "aclImdb/test", batch_size=batch_size
)
train_ds_3 = keras.utils.text_dataset_from_directory(
    "aclImdb/train3", batch_size=batch_size
)

text_only_train_ds_3 = train_ds_3.map(lambda x, y: x)
text_vectorization.adapt(text_only_train_ds_3)

"""# Establishing another model using an embedding layer training with 998 samples

I'm still keeping the validation samples to 10,000, cutting off reviews after 150 words, and considering only the top 10,000 words for this model and the next model.
"""

from tensorflow.keras import layers

max_length = 150
max_tokens = 10000
text_vectorization = layers.TextVectorization(
    max_tokens=max_tokens,
    output_mode="int",
    output_sequence_length=max_length,
)
text_vectorization.adapt(text_only_train_ds_3)

int_train_ds = train_ds.map(
    lambda x, y: (text_vectorization(x), y),
    num_parallel_calls=4)
int_val_ds = val_ds.map(
    lambda x, y: (text_vectorization(x), y),
    num_parallel_calls=4)
int_test_ds = test_ds.map(
    lambda x, y: (text_vectorization(x), y),
    num_parallel_calls=4)
int_train_ds_3 = train_ds_3.map(
    lambda x, y: (text_vectorization(x), y),
    num_parallel_calls=4)

embedding_layer = layers.Embedding(input_dim=max_tokens, output_dim=256)

import tensorflow as tf
inputs = keras.Input(shape=(None,), dtype="int64")
embedded = layers.Embedding(
    input_dim=max_tokens, output_dim=256, mask_zero=True)(inputs)
x = layers.Bidirectional(layers.LSTM(32))(embedded)
x = layers.Dropout(0.5)(x)
outputs = layers.Dense(1, activation="sigmoid")(x)
model = keras.Model(inputs, outputs)
model.compile(optimizer="rmsprop",
              loss="binary_crossentropy",
              metrics=["accuracy"])
model.summary()

callbacks = [
    keras.callbacks.ModelCheckpoint("embeddings_bidir_gru_with_masking_2.keras",
                                    save_best_only=True)
]
model.fit(int_train_ds_3, validation_data=int_val_ds, epochs=10, callbacks=callbacks)
model = keras.models.load_model("embeddings_bidir_gru_with_masking_2.keras")
print(f"Test acc: {model.evaluate(int_test_ds)[1]:.3f}")

"""Training a model with an embedding layer (trained from scratch) yields a validation accuracy of ~76.8% (at best) and a test accuracy of ~74.7% when using 998 training samples. This is a significant improvement in performance compared to only training on 100 samples.

I'm now going to see how this performance compares to a model using a pretrained word embedding layer with 998 training samples.

# Establishing another model using pretrained word embedding with 998 training samples
"""

inputs = keras.Input(shape=(None,), dtype="int64")
embedded = embedding_layer(inputs)
x = layers.Bidirectional(layers.LSTM(32))(embedded)
x = layers.Dropout(0.5)(x)
outputs = layers.Dense(1, activation="sigmoid")(x)
model = keras.Model(inputs, outputs)
model.compile(optimizer="rmsprop",
              loss="binary_crossentropy",
              metrics=["accuracy"])
model.summary()

callbacks = [
    keras.callbacks.ModelCheckpoint("glove_embeddings_sequence_model_2.keras",
                                    save_best_only=True)
]
model.fit(int_train_ds_3, validation_data=int_val_ds, epochs=10, callbacks=callbacks)
model = keras.models.load_model("glove_embeddings_sequence_model_2.keras")
print(f"Test acc: {model.evaluate(int_test_ds)[1]:.3f}")

"""Training a model with a pretrained word embedding layer yields a validation accuracy of ~75.6% (at best) and a test accuracy of ~73.8% when using 998 training samples. Again, this is a significant improvement in performance compared to only training on 100 samples.

This model is still performing comparably to the model using an embedding layer trained from scratch, but appears to be performing worse overall when using 998 training samples (based on running both models multiple times at this training sample size). It appears it doesn't take a very large training sample size for the embedding layer model to outperform the pretrained word embedding layer model.

I'm now going to decrease the training samples to about 500 to see if that is sufficient to allow the embedding layer model to outperform the pretrained embedding layer model.

# Decreasing the training samples to 486 (about 500)
"""

import os, pathlib, shutil, random

base_dir = pathlib.Path("aclImdb")
val_dir = base_dir / "val"
train_dir = base_dir / "train"
train_dir_4 = base_dir / "train4"

for category in ("neg", "pos"):
    os.makedirs(train_dir_4 / category)
    files = os.listdir(train_dir / category)
    random.Random(1337).shuffle(files)
    num_train_samples_3 = int(0.035 * len(files))
    train_files_3 = files[-num_train_samples_3:]
    for fname in train_files_3:
        shutil.move(train_dir / category / fname,
                    train_dir_4 / category / fname)

from tensorflow import keras
batch_size = 32

train_ds = keras.utils.text_dataset_from_directory(
    "aclImdb/train", batch_size=batch_size
)
val_ds = keras.utils.text_dataset_from_directory(
    "aclImdb/val", batch_size=batch_size
)
test_ds = keras.utils.text_dataset_from_directory(
    "aclImdb/test", batch_size=batch_size
)
train_ds_4 = keras.utils.text_dataset_from_directory(
    "aclImdb/train4", batch_size=batch_size
)

text_only_train_ds_4 = train_ds_4.map(lambda x, y: x)
text_vectorization.adapt(text_only_train_ds_4)

"""# Establishing another model using an embedding layer with around 500 training samples

Again, I'm still keeping the validation samples to 10,000, cutting off reviews after 150 words, and considering only the top 10,000 words for this model and the next model.
"""

from tensorflow.keras import layers

max_length = 150
max_tokens = 10000
text_vectorization = layers.TextVectorization(
    max_tokens=max_tokens,
    output_mode="int",
    output_sequence_length=max_length,
)
text_vectorization.adapt(text_only_train_ds_4)

int_train_ds = train_ds.map(
    lambda x, y: (text_vectorization(x), y),
    num_parallel_calls=4)
int_val_ds = val_ds.map(
    lambda x, y: (text_vectorization(x), y),
    num_parallel_calls=4)
int_test_ds = test_ds.map(
    lambda x, y: (text_vectorization(x), y),
    num_parallel_calls=4)
int_train_ds_4 = train_ds_4.map(
    lambda x, y: (text_vectorization(x), y),
    num_parallel_calls=4)

embedding_layer = layers.Embedding(input_dim=max_tokens, output_dim=256)

import tensorflow as tf
inputs = keras.Input(shape=(None,), dtype="int64")
embedded = layers.Embedding(
    input_dim=max_tokens, output_dim=256, mask_zero=True)(inputs)
x = layers.Bidirectional(layers.LSTM(32))(embedded)
x = layers.Dropout(0.5)(x)
outputs = layers.Dense(1, activation="sigmoid")(x)
model = keras.Model(inputs, outputs)
model.compile(optimizer="rmsprop",
              loss="binary_crossentropy",
              metrics=["accuracy"])
model.summary()

callbacks = [
    keras.callbacks.ModelCheckpoint("embeddings_bidir_gru_with_masking_3.keras",
                                    save_best_only=True)
]
model.fit(int_train_ds_4, validation_data=int_val_ds, epochs=10, callbacks=callbacks)
model = keras.models.load_model("embeddings_bidir_gru_with_masking_3.keras")
print(f"Test acc: {model.evaluate(int_test_ds)[1]:.3f}")

"""Training a model with an embedding layer (trained from scratch) yields a validation accuracy of ~72.9% (at best) and a test accuracy of ~71.1% when using ~500 training samples. This is again a significant improvement in performance compared to only training on 100 samples, but not as good as training on close to 1,000 samples.

I'm now going to see how this performance compares to a model using a pretrained word embedding layer with around 500 training samples to see if only ~500 training samples is sufficient to enable the embedding layer model trained from scratch to outperform the pretrained word embedding model.

# Establishing a model using pretrained word embedding with around 500 training samples
"""

inputs = keras.Input(shape=(None,), dtype="int64")
embedded = embedding_layer(inputs)
x = layers.Bidirectional(layers.LSTM(32))(embedded)
x = layers.Dropout(0.5)(x)
outputs = layers.Dense(1, activation="sigmoid")(x)
model = keras.Model(inputs, outputs)
model.compile(optimizer="rmsprop",
              loss="binary_crossentropy",
              metrics=["accuracy"])
model.summary()

callbacks = [
    keras.callbacks.ModelCheckpoint("glove_embeddings_sequence_model_3.keras",
                                    save_best_only=True)
]
model.fit(int_train_ds_4, validation_data=int_val_ds, epochs=10, callbacks=callbacks)
model = keras.models.load_model("glove_embeddings_sequence_model_3.keras")
print(f"Test acc: {model.evaluate(int_test_ds)[1]:.3f}")

"""Training a model with a pretrained embedding layer yields a validation accuracy of ~71.4% (at best) and a test accuracy of ~68.8% when using ~500 training samples. This is again a significant improvement in performance compared to only training on 100 samples, but also not as good as training on close to 1,000 samples.

It appears that the model with the embedding layer trained from scratch overall outperforms the model with the pretrained layer, even with only ~500 training samples. It appears that it doesn't take a large training sample for the model with the embedding layer trained from scratch to outperform the model with the pretrained embedding layer on this dataset.

# Conclusion
"""

import pandas

data_models = [['Bag-of-Words', 100], ['Embed from Scratch', 100], ['Pretrain Embed', 100], ['Embed from Scratch', 998], ['Pretrain Embed', 998], ['Embed from Scratch', 486], ['Pretrain Embed', 486]]

df = pandas.DataFrame(data_models, index=[1,2,3,4,5,6,7], columns=['Model Type', 'Train Sample Size'])
print(df)

data_avgs = [[71.2, 70.9], [53.0, 53.7], [54.0, 54.7], [76.8, 74.7], [75.6, 73.8], [72.9, 71.1], [71.4, 68.8]]

df2 = pandas.DataFrame(data_avgs, index=[1,2,3,4,5,6,7], columns=['Validation Accuracy', 'Test Accuracy'])
print(df2)

plot = df2.plot.bar(ylim=(52.0, 78.0), rot= 0, title="Model Performance")
plot.set_xlabel("Model Number")
plot.set_ylabel("Accuracy (%)")
plot

"""## In conclusion, when using only 100 training samples, the model with a pretrained embedding layer performed very comparably to the model with an embedding layer trained from scratch, and at times, performed better than the embedding layer model trained from scratch. The model with a pretrained embedding layer yielded about 54.0% validation accuracy and 54.7% test accuracy compared to the embedding model trained from scratch which yielded about 53.0% validation accuracy 53.7% test accuracy.

## When increasing the training sample size to 998, the model with an embedding layer trained from scratch outperformed the model with a pretrained embedding layer. The model with an embedding layer trained from scratch yielded a validation accuracy of ~76.8% and a test accuracy of ~74.7% compared to the model with a pretrained embedding layer which resulted in a validation accuracy of ~75.6% and a test accuracy of ~73.8%. Both of these models performed significantly better at 1,000 training samples vs. the initial 100 training samples.

## After seeing that the model with an embedding layer trained from scratch outperformed the model with a pretrained embedding layer at around 1,000 training samples, I decreased the number of training samples to 486 (roughly half) to see if this was enough samples to allow the embedding layer trained from scratch to outperform the pretrained embedding layer. It appears that roughly 500 training samples is enough for the model with the embedding layer trained from scratch to perform better (validation accuracy of ~72.9%, test accuracy of ~71.1%). This is in comparison to the model with a pretrained embedding layer, which resulted in a validation accuracy of ~71.4% and a test accuracy of ~68.8%. It appears that it doesn't take a large training sample for the model with the embedding layer trained from scratch to outperform the model with the pretrained embedding layer on this dataset.

## Lastly, the bag-of-word model displayed great potential, performing surprisingly well at only 100 training samples (71.2% validation accuracy, 70.9% test accuracy). Because this assignment was focused on models using embedding layers trained from scratch vs. models using pretrained embedding layers, I didn't attempt the bag-of-word approach on larger sample sizes, but I'm confident it would perform very well on a larger sample given how well it performed at only 100 training samples.
"""