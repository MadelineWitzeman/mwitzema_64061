{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "V100",
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Assignment 4: Text and Sequence Data\n",
        "# BA 64061-003\n",
        "# Madeline Witzeman"
      ],
      "metadata": {
        "id": "H0ae2tCON1u2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loading and Preparing the IMDb dataset"
      ],
      "metadata": {
        "id": "voMcC1NAknNi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!curl -O https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
        "!tar -xf aclImdb_v1.tar.gz"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "df5RyE7CkywG",
        "outputId": "e3e90b80-7db1-4e7f-8860-06a6e75b0b1c"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 80.2M  100 80.2M    0     0  3446k      0  0:00:23  0:00:23 --:--:-- 5264k\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -r aclImdb/train/unsup"
      ],
      "metadata": {
        "id": "zmMoFSW3k7KS"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Inspecting some of the data prior to modeling:"
      ],
      "metadata": {
        "id": "7hORN3QjlmW1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!cat aclImdb/train/pos/4077_10.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J7JqfftIk789",
        "outputId": "8307431b-3340-4815-a111-171135524ca6"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I first saw this back in the early 90s on UK TV, i did like it then but i missed the chance to tape it, many years passed but the film always stuck with me and i lost hope of seeing it TV again, the main thing that stuck with me was the end, the hole castle part really touched me, its easy to watch, has a great story, great music, the list goes on and on, its OK me saying how good it is but everyone will take there own best bits away with them once they have seen it, yes the animation is top notch and beautiful to watch, it does show its age in a very few parts but that has now become part of it beauty, i am so glad it has came out on DVD as it is one of my top 10 films of all time. Buy it or rent it just see it, best viewing is at night alone with drink and food in reach so you don't have to stop the film.<br /><br />Enjoy"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Based on the assignment instructions, I am validating on 10,000 samples (#3) and training on 100 samples (#2):"
      ],
      "metadata": {
        "id": "oAnZlO8Fn8-z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os, pathlib, shutil, random\n",
        "\n",
        "base_dir = pathlib.Path(\"aclImdb\")\n",
        "val_dir = base_dir / \"val\"\n",
        "train_dir = base_dir / \"train\"\n",
        "train_dir_2 = base_dir / \"train2\"\n",
        "for category in (\"neg\", \"pos\"):\n",
        "    os.makedirs(val_dir / category)\n",
        "    files = os.listdir(train_dir / category)\n",
        "    random.Random(1337).shuffle(files)\n",
        "    num_val_samples = int(0.4 * len(files))\n",
        "    val_files = files[-num_val_samples:]\n",
        "    for fname in val_files:\n",
        "        shutil.move(train_dir / category / fname,\n",
        "                    val_dir / category / fname)\n",
        "\n",
        "for category in (\"neg\", \"pos\"):\n",
        "    os.makedirs(train_dir_2 / category)\n",
        "    files = os.listdir(train_dir / category)\n",
        "    random.Random(1337).shuffle(files)\n",
        "    num_train_samples = int(0.0067 * len(files))\n",
        "    train_files = files[-num_train_samples:]\n",
        "    for fname in train_files:\n",
        "        shutil.move(train_dir / category / fname,\n",
        "                    train_dir_2 / category / fname)"
      ],
      "metadata": {
        "id": "C4JI9NFFQBVy"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow import keras\n",
        "batch_size = 32\n",
        "\n",
        "train_ds = keras.utils.text_dataset_from_directory(\n",
        "    \"aclImdb/train\", batch_size=batch_size\n",
        ")\n",
        "val_ds = keras.utils.text_dataset_from_directory(\n",
        "    \"aclImdb/val\", batch_size=batch_size\n",
        ")\n",
        "test_ds = keras.utils.text_dataset_from_directory(\n",
        "    \"aclImdb/test\", batch_size=batch_size\n",
        ")\n",
        "train_ds_2 = keras.utils.text_dataset_from_directory(\n",
        "    \"aclImdb/train2\", batch_size=batch_size\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MD3froGwk_rZ",
        "outputId": "d9531d70-0f78-475e-907d-512d4b92f51a"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 14900 files belonging to 2 classes.\n",
            "Found 10000 files belonging to 2 classes.\n",
            "Found 25000 files belonging to 2 classes.\n",
            "Found 100 files belonging to 2 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Displaying the shapes and data types of the first batch"
      ],
      "metadata": {
        "id": "QOOUgfGjlD_1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for inputs, targets in train_ds_2:\n",
        "    print(\"inputs.shape:\", inputs.shape)\n",
        "    print(\"inputs.dtype:\", inputs.dtype)\n",
        "    print(\"targets.shape:\", targets.shape)\n",
        "    print(\"targets.dtype:\", targets.dtype)\n",
        "    print(\"inputs[0]:\", inputs[0])\n",
        "    print(\"targets[0]:\", targets[0])\n",
        "    break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V1WHoOv8lBd4",
        "outputId": "fded5631-8d25-4c2a-fab5-462e942331e7"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "inputs.shape: (32,)\n",
            "inputs.dtype: <dtype: 'string'>\n",
            "targets.shape: (32,)\n",
            "targets.dtype: <dtype: 'int32'>\n",
            "inputs[0]: tf.Tensor(b\"This is why i so love this website ! I saw this film in the 1980's on British television. Over the years it is one i have wished i knew more about as it has stayed with me as one of the single most extraordinary things i have ever seen in my life. With barely a few key words to remember it by, i traced the film here, and much information, including the fact it's about to become an off-Broadway musical !<br /><br />Interestingly, unlike the previous comment maker, i do not remember finding this film sad, or exploitative. On the contrary, the extraordinary relationship between the mother and daughter stuck in the mind as a testimony of great strength, honour and dignity. Ironic you may think, considering the squalor of their lives. Maybe it's because i live in Britain, where fading grandeur has an established language in the lives of old money, where squalor is often tolerated as evidence of good breeding; I saw it as a rare and unique portrayal of enormous spirit, deep and profound humour, whose utterly fragile and delicately balanced fabric gave it poise and respect. In a way i was sorry to see it being discussed as a 'cult'. Over the years, as it faded in my mind, it shone the brightest, above all others as a one off brilliant & outstanding televisual experience. It was such a deeply private expose, it seems odd to think of it becoming so public as to be a New York musical. But perhaps somewhere, the daughter will be amused by such an outcome. It is she who will have the last laugh maybe..(They made a musical out of her before you Jackie O' )\", shape=(), dtype=string)\n",
            "targets[0]: tf.Tensor(1, shape=(), dtype=int32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Using the bag-of-words with multi-hot binary vectors approach to establish a baseline"
      ],
      "metadata": {
        "id": "dNm0BZRYnnqL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preprocessing the datasets with a TextVectorization layer. Restricting the vocabulary to the top 10,000 words based on assignment instructions (#4):"
      ],
      "metadata": {
        "id": "t1nK2yrMowNW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import TextVectorization\n",
        "text_vectorization = TextVectorization(\n",
        "    output_mode=\"int\",\n",
        ")"
      ],
      "metadata": {
        "id": "rxHEP4dfpqCf"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_vectorization = TextVectorization(\n",
        "    max_tokens=10000,\n",
        "    output_mode=\"multi_hot\",\n",
        ")\n",
        "text_only_train_ds = train_ds.map(lambda x, y: x)\n",
        "text_vectorization.adapt(text_only_train_ds)\n",
        "\n",
        "binary_1gram_train_ds = train_ds.map(\n",
        "    lambda x, y: (text_vectorization(x), y),\n",
        "    num_parallel_calls=4)\n",
        "binary_1gram_val_ds = val_ds.map(\n",
        "    lambda x, y: (text_vectorization(x), y),\n",
        "    num_parallel_calls=4)\n",
        "binary_1gram_test_ds = test_ds.map(\n",
        "    lambda x, y: (text_vectorization(x), y),\n",
        "    num_parallel_calls=4)\n",
        "binary_1gram_train_ds2 = train_ds_2.map(\n",
        "    lambda x, y: (text_vectorization(x), y),\n",
        "    num_parallel_calls=4)"
      ],
      "metadata": {
        "id": "TBHtjW4goXId"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Inspecting the output of the binary unigram dataset"
      ],
      "metadata": {
        "id": "4-LW2P8Mp72m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for inputs, targets in binary_1gram_train_ds2:\n",
        "    print(\"inputs.shape:\", inputs.shape)\n",
        "    print(\"inputs.dtype:\", inputs.dtype)\n",
        "    print(\"targets.shape:\", targets.shape)\n",
        "    print(\"targets.dtype:\", targets.dtype)\n",
        "    print(\"inputs[0]:\", inputs[0])\n",
        "    print(\"targets[0]:\", targets[0])\n",
        "    break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dP3UPUFHqAga",
        "outputId": "ad728b0c-2ca0-485d-d73e-611e23b359bf"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "inputs.shape: (32, 10000)\n",
            "inputs.dtype: <dtype: 'float32'>\n",
            "targets.shape: (32,)\n",
            "targets.dtype: <dtype: 'int32'>\n",
            "inputs[0]: tf.Tensor([1. 1. 1. ... 0. 0. 0.], shape=(10000,), dtype=float32)\n",
            "targets[0]: tf.Tensor(0, shape=(), dtype=int32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The model-building utility"
      ],
      "metadata": {
        "id": "RKEVH88KqJZq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "def get_model(max_tokens=10000, hidden_dim=16):\n",
        "    inputs = keras.Input(shape=(max_tokens,))\n",
        "    x = layers.Dense(hidden_dim, activation=\"relu\")(inputs)\n",
        "    x = layers.Dropout(0.5)(x)\n",
        "    outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
        "    model = keras.Model(inputs, outputs)\n",
        "    model.compile(optimizer=\"rmsprop\",\n",
        "                  loss=\"binary_crossentropy\",\n",
        "                  metrics=[\"accuracy\"])\n",
        "    return model"
      ],
      "metadata": {
        "id": "FyHDODqIqPyb"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training and testing the binary unigram model"
      ],
      "metadata": {
        "id": "K87X7SUxqesj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = get_model()\n",
        "model.summary()\n",
        "callbacks = [\n",
        "    keras.callbacks.ModelCheckpoint(\"binary_1gram.keras\",\n",
        "                                    save_best_only=True)\n",
        "]\n",
        "model.fit(binary_1gram_train_ds2.cache(),\n",
        "          validation_data=binary_1gram_val_ds.cache(),\n",
        "          epochs=10,\n",
        "          callbacks=callbacks)\n",
        "model = keras.models.load_model(\"binary_1gram.keras\")\n",
        "print(f\"Test acc: {model.evaluate(binary_1gram_test_ds)[1]:.3f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v2oqJWdtqgFf",
        "outputId": "02fd16d7-0c97-4d62-fb6d-fff980c3b1ae"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_1 (InputLayer)        [(None, 10000)]           0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 16)                160016    \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 16)                0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 1)                 17        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 160033 (625.13 KB)\n",
            "Trainable params: 160033 (625.13 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "Epoch 1/10\n",
            "4/4 [==============================] - 4s 311ms/step - loss: 0.7049 - accuracy: 0.4800 - val_loss: 0.6833 - val_accuracy: 0.5618\n",
            "Epoch 2/10\n",
            "4/4 [==============================] - 1s 211ms/step - loss: 0.6024 - accuracy: 0.6900 - val_loss: 0.6651 - val_accuracy: 0.6375\n",
            "Epoch 3/10\n",
            "4/4 [==============================] - 1s 215ms/step - loss: 0.5041 - accuracy: 0.8600 - val_loss: 0.6491 - val_accuracy: 0.6664\n",
            "Epoch 4/10\n",
            "4/4 [==============================] - 1s 215ms/step - loss: 0.4365 - accuracy: 0.9000 - val_loss: 0.6359 - val_accuracy: 0.6756\n",
            "Epoch 5/10\n",
            "4/4 [==============================] - 1s 221ms/step - loss: 0.4108 - accuracy: 0.9200 - val_loss: 0.6231 - val_accuracy: 0.6930\n",
            "Epoch 6/10\n",
            "4/4 [==============================] - 1s 211ms/step - loss: 0.3386 - accuracy: 0.9300 - val_loss: 0.6124 - val_accuracy: 0.7087\n",
            "Epoch 7/10\n",
            "4/4 [==============================] - 1s 212ms/step - loss: 0.3066 - accuracy: 0.9500 - val_loss: 0.6048 - val_accuracy: 0.6953\n",
            "Epoch 8/10\n",
            "4/4 [==============================] - 1s 211ms/step - loss: 0.2806 - accuracy: 0.9600 - val_loss: 0.5964 - val_accuracy: 0.7063\n",
            "Epoch 9/10\n",
            "4/4 [==============================] - 1s 207ms/step - loss: 0.2298 - accuracy: 0.9700 - val_loss: 0.5901 - val_accuracy: 0.7118\n",
            "Epoch 10/10\n",
            "4/4 [==============================] - 1s 210ms/step - loss: 0.1946 - accuracy: 0.9900 - val_loss: 0.5896 - val_accuracy: 0.6931\n",
            "782/782 [==============================] - 2s 3ms/step - loss: 0.5954 - accuracy: 0.6862\n",
            "Test acc: 0.686\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "By only using 100 training samples, this approach yields a validation accuracy of ~71.2% (at best) and a test accuracy of ~70.9%."
      ],
      "metadata": {
        "id": "5vId8gi-qnqm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Establishing a second model using an embedding layer"
      ],
      "metadata": {
        "id": "ZkFbSdRyraQX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preparing integer sequence datasets. Based on the assignment instructions, I'm still limiting the vocabulary to the top 10,000 words (#4) and now cutting off the reviews after 150 words (#1):"
      ],
      "metadata": {
        "id": "KbLUPjRGsmif"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras import layers\n",
        "\n",
        "max_length = 150\n",
        "max_tokens = 10000\n",
        "text_vectorization = layers.TextVectorization(\n",
        "    max_tokens=max_tokens,\n",
        "    output_mode=\"int\",\n",
        "    output_sequence_length=max_length,\n",
        ")\n",
        "text_vectorization.adapt(text_only_train_ds)\n",
        "\n",
        "int_train_ds = train_ds.map(\n",
        "    lambda x, y: (text_vectorization(x), y),\n",
        "    num_parallel_calls=4)\n",
        "int_val_ds = val_ds.map(\n",
        "    lambda x, y: (text_vectorization(x), y),\n",
        "    num_parallel_calls=4)\n",
        "int_test_ds = test_ds.map(\n",
        "    lambda x, y: (text_vectorization(x), y),\n",
        "    num_parallel_calls=4)\n",
        "int_train_ds_2 = train_ds_2.map(\n",
        "    lambda x, y: (text_vectorization(x), y),\n",
        "    num_parallel_calls=4)"
      ],
      "metadata": {
        "id": "EETAdUVOtGAs"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Building a sequence model with an embedding layer trained from scratch with masking enabled"
      ],
      "metadata": {
        "id": "tc5K3eBftpMb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_layer = layers.Embedding(input_dim=max_tokens, output_dim=256)"
      ],
      "metadata": {
        "id": "33b7UQyEt-Oz"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "inputs = keras.Input(shape=(None,), dtype=\"int64\")\n",
        "embedded = layers.Embedding(\n",
        "    input_dim=max_tokens, output_dim=256, mask_zero=True)(inputs)\n",
        "x = layers.Bidirectional(layers.LSTM(32))(embedded)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
        "model = keras.Model(inputs, outputs)\n",
        "model.compile(optimizer=\"rmsprop\",\n",
        "              loss=\"binary_crossentropy\",\n",
        "              metrics=[\"accuracy\"])\n",
        "model.summary()\n",
        "\n",
        "callbacks = [\n",
        "    keras.callbacks.ModelCheckpoint(\"embeddings_bidir_gru_with_masking.keras\",\n",
        "                                    save_best_only=True)\n",
        "]\n",
        "model.fit(int_train_ds_2, validation_data=int_val_ds, epochs=10, callbacks=callbacks)\n",
        "model = keras.models.load_model(\"embeddings_bidir_gru_with_masking.keras\")\n",
        "print(f\"Test acc: {model.evaluate(int_test_ds)[1]:.3f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jWNjvNlWt3r2",
        "outputId": "74bd6b32-072c-4a1f-b0fd-b819441f7ddd"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_7\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_8 (InputLayer)        [(None, None)]            0         \n",
            "                                                                 \n",
            " embedding_7 (Embedding)     (None, None, 256)         2560000   \n",
            "                                                                 \n",
            " bidirectional_6 (Bidirecti  (None, 64)                73984     \n",
            " onal)                                                           \n",
            "                                                                 \n",
            " dropout_7 (Dropout)         (None, 64)                0         \n",
            "                                                                 \n",
            " dense_8 (Dense)             (None, 1)                 65        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2634049 (10.05 MB)\n",
            "Trainable params: 2634049 (10.05 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "Epoch 1/10\n",
            "4/4 [==============================] - 15s 2s/step - loss: 0.6943 - accuracy: 0.5200 - val_loss: 0.6932 - val_accuracy: 0.5026\n",
            "Epoch 2/10\n",
            "4/4 [==============================] - 2s 799ms/step - loss: 0.6854 - accuracy: 0.6200 - val_loss: 0.6933 - val_accuracy: 0.4956\n",
            "Epoch 3/10\n",
            "4/4 [==============================] - 2s 789ms/step - loss: 0.6764 - accuracy: 0.7500 - val_loss: 0.6936 - val_accuracy: 0.4960\n",
            "Epoch 4/10\n",
            "4/4 [==============================] - 3s 877ms/step - loss: 0.6646 - accuracy: 0.8000 - val_loss: 0.6939 - val_accuracy: 0.4957\n",
            "Epoch 5/10\n",
            "4/4 [==============================] - 3s 852ms/step - loss: 0.6498 - accuracy: 0.8500 - val_loss: 0.6943 - val_accuracy: 0.4990\n",
            "Epoch 6/10\n",
            "4/4 [==============================] - 3s 900ms/step - loss: 0.6343 - accuracy: 0.8900 - val_loss: 0.6950 - val_accuracy: 0.4980\n",
            "Epoch 7/10\n",
            "4/4 [==============================] - 3s 888ms/step - loss: 0.5993 - accuracy: 0.9100 - val_loss: 0.6976 - val_accuracy: 0.4910\n",
            "Epoch 8/10\n",
            "4/4 [==============================] - 3s 888ms/step - loss: 0.5489 - accuracy: 0.9300 - val_loss: 0.7372 - val_accuracy: 0.4977\n",
            "Epoch 9/10\n",
            "4/4 [==============================] - 3s 835ms/step - loss: 0.4756 - accuracy: 0.8200 - val_loss: 0.7973 - val_accuracy: 0.5014\n",
            "Epoch 10/10\n",
            "4/4 [==============================] - 3s 855ms/step - loss: 0.4618 - accuracy: 0.8700 - val_loss: 0.7140 - val_accuracy: 0.5075\n",
            "782/782 [==============================] - 11s 8ms/step - loss: 0.6934 - accuracy: 0.4938\n",
            "Test acc: 0.494\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "I enabled masking with the embedded layer since it improved model performance in the textbook. This approach yields a validation accuracy of ~53.0% (at best) and a test accuracy of ~53.7%, which is worse than the baseline binary unigram model. I'm now going to see how it compares to the pretrained word embedding model training on only 100 samples."
      ],
      "metadata": {
        "id": "XYieRH4tu6D4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Establishing a third model using pretrained word embedding"
      ],
      "metadata": {
        "id": "EJZN7AwRysri"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
        "!unzip -q glove.6B.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D5ZaVgfXvVPw",
        "outputId": "f3abf492-6cef-4b30-d2d4-5cb9590bb393"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-04-25 04:38:58--  http://nlp.stanford.edu/data/glove.6B.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://nlp.stanford.edu/data/glove.6B.zip [following]\n",
            "--2024-04-25 04:38:58--  https://nlp.stanford.edu/data/glove.6B.zip\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n",
            "--2024-04-25 04:38:59--  https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
            "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
            "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 862182613 (822M) [application/zip]\n",
            "Saving to: ‘glove.6B.zip’\n",
            "\n",
            "glove.6B.zip        100%[===================>] 822.24M  5.12MB/s    in 2m 40s  \n",
            "\n",
            "2024-04-25 04:41:39 (5.14 MB/s) - ‘glove.6B.zip’ saved [862182613/862182613]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Parsing the GloVe word-embeddings file"
      ],
      "metadata": {
        "id": "5mSHelU21sTe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "path_to_glove_file = \"glove.6B.100d.txt\"\n",
        "\n",
        "embeddings_index = {}\n",
        "with open(path_to_glove_file) as f:\n",
        "    for line in f:\n",
        "        word, coefs = line.split(maxsplit=1)\n",
        "        coefs = np.fromstring(coefs, \"f\", sep=\" \")\n",
        "        embeddings_index[word] = coefs\n",
        "\n",
        "print(f\"Found {len(embeddings_index)} word vectors.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vC5SXzrQ1uIM",
        "outputId": "82d64040-27de-4ce2-c69e-f492c8684b82"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 400000 word vectors.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preparing the GloVe word-embeddings matrix"
      ],
      "metadata": {
        "id": "N637dFtx1xeY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_dim = 100\n",
        "\n",
        "vocabulary = text_vectorization.get_vocabulary()\n",
        "word_index = dict(zip(vocabulary, range(len(vocabulary))))\n",
        "\n",
        "embedding_matrix = np.zeros((max_tokens, embedding_dim))\n",
        "for word, i in word_index.items():\n",
        "    if i < max_tokens:\n",
        "        embedding_vector = embeddings_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        embedding_matrix[i] = embedding_vector"
      ],
      "metadata": {
        "id": "UMMd2Xev11A0"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_layer = layers.Embedding(\n",
        "    max_tokens,\n",
        "    embedding_dim,\n",
        "    embeddings_initializer=keras.initializers.Constant(embedding_matrix),\n",
        "    trainable=False,\n",
        "    mask_zero=True,\n",
        ")"
      ],
      "metadata": {
        "id": "N47QCume1-CA"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Building a model that uses a pretrained Embedding layer"
      ],
      "metadata": {
        "id": "KXoxTKAK13hD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = keras.Input(shape=(None,), dtype=\"int64\")\n",
        "embedded = embedding_layer(inputs)\n",
        "x = layers.Bidirectional(layers.LSTM(32))(embedded)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
        "model = keras.Model(inputs, outputs)\n",
        "model.compile(optimizer=\"rmsprop\",\n",
        "              loss=\"binary_crossentropy\",\n",
        "              metrics=[\"accuracy\"])\n",
        "model.summary()\n",
        "\n",
        "callbacks = [\n",
        "    keras.callbacks.ModelCheckpoint(\"glove_embeddings_sequence_model.keras\",\n",
        "                                    save_best_only=True)\n",
        "]\n",
        "model.fit(int_train_ds_2, validation_data=int_val_ds, epochs=10, callbacks=callbacks)\n",
        "model = keras.models.load_model(\"glove_embeddings_sequence_model.keras\")\n",
        "print(f\"Test acc: {model.evaluate(int_test_ds)[1]:.3f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gt0BWK5q20Qz",
        "outputId": "16af8fe4-b8b0-49d7-d27e-5ebcf29030fb"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_8\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_9 (InputLayer)        [(None, None)]            0         \n",
            "                                                                 \n",
            " embedding_5 (Embedding)     (None, None, 256)         2560000   \n",
            "                                                                 \n",
            " bidirectional_7 (Bidirecti  (None, 64)                73984     \n",
            " onal)                                                           \n",
            "                                                                 \n",
            " dropout_8 (Dropout)         (None, 64)                0         \n",
            "                                                                 \n",
            " dense_9 (Dense)             (None, 1)                 65        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2634049 (10.05 MB)\n",
            "Trainable params: 2634049 (10.05 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "Epoch 1/10\n",
            "4/4 [==============================] - 6s 1s/step - loss: 0.6949 - accuracy: 0.4700 - val_loss: 0.6933 - val_accuracy: 0.5005\n",
            "Epoch 2/10\n",
            "4/4 [==============================] - 2s 794ms/step - loss: 0.6835 - accuracy: 0.5400 - val_loss: 0.6941 - val_accuracy: 0.5010\n",
            "Epoch 3/10\n",
            "4/4 [==============================] - 2s 783ms/step - loss: 0.6748 - accuracy: 0.5900 - val_loss: 0.6945 - val_accuracy: 0.4936\n",
            "Epoch 4/10\n",
            "4/4 [==============================] - 2s 770ms/step - loss: 0.6633 - accuracy: 0.8100 - val_loss: 0.6973 - val_accuracy: 0.4956\n",
            "Epoch 5/10\n",
            "4/4 [==============================] - 2s 711ms/step - loss: 0.6462 - accuracy: 0.7400 - val_loss: 0.6983 - val_accuracy: 0.4807\n",
            "Epoch 6/10\n",
            "4/4 [==============================] - 2s 723ms/step - loss: 0.6137 - accuracy: 0.8700 - val_loss: 0.7063 - val_accuracy: 0.4752\n",
            "Epoch 7/10\n",
            "4/4 [==============================] - 2s 719ms/step - loss: 0.5657 - accuracy: 0.8900 - val_loss: 0.7221 - val_accuracy: 0.4715\n",
            "Epoch 8/10\n",
            "4/4 [==============================] - 2s 741ms/step - loss: 0.5018 - accuracy: 0.8500 - val_loss: 0.7260 - val_accuracy: 0.4975\n",
            "Epoch 9/10\n",
            "4/4 [==============================] - 2s 744ms/step - loss: 0.4074 - accuracy: 0.9200 - val_loss: 0.7184 - val_accuracy: 0.5006\n",
            "Epoch 10/10\n",
            "4/4 [==============================] - 2s 752ms/step - loss: 0.3595 - accuracy: 0.9800 - val_loss: 0.7476 - val_accuracy: 0.4885\n",
            "782/782 [==============================] - 7s 7ms/step - loss: 0.6932 - accuracy: 0.5030\n",
            "Test acc: 0.503\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This approach yields a validation accuracy of ~54.0% (at best) and a test accuracy of ~54.7%, which is also worse than the baseline binary unigram model performance. This model performed very comparably to the model using an embedding layer trained from scratch. At times, the model with a pretrained word embedding layer performed better than the model with an embedding layer trained from scratch, and vice versa.\n",
        "\n",
        "This makes sense given I am only training on a sample size of 100 movie reviews. When there isn't a large enough training sample to learn from the dataset itself, pretrained embedding layers can help improve model performance.\n",
        "\n",
        "I'm going to alter the training sample size to see at what point the model utilizing the embedding layer trained from scracth notably outperforms the model utilizing the pretrained embedding layer."
      ],
      "metadata": {
        "id": "QdCtHbAe3-q_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Increasing the training samples to 998 (almost 1,000)"
      ],
      "metadata": {
        "id": "GXg5JQrQ4Z_B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os, pathlib, shutil, random\n",
        "\n",
        "base_dir = pathlib.Path(\"aclImdb\")\n",
        "val_dir = base_dir / \"val\"\n",
        "train_dir = base_dir / \"train\"\n",
        "train_dir_3 = base_dir / \"train3\"\n",
        "\n",
        "for category in (\"neg\", \"pos\"):\n",
        "    os.makedirs(train_dir_3 / category)\n",
        "    files = os.listdir(train_dir / category)\n",
        "    random.Random(1337).shuffle(files)\n",
        "    num_train_samples_2 = int(0.067 * len(files))\n",
        "    train_files_2 = files[-num_train_samples_2:]\n",
        "    for fname in train_files_2:\n",
        "        shutil.move(train_dir / category / fname,\n",
        "                    train_dir_3 / category / fname)"
      ],
      "metadata": {
        "id": "6FpfzfLXvtH6"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow import keras\n",
        "batch_size = 32\n",
        "\n",
        "train_ds = keras.utils.text_dataset_from_directory(\n",
        "    \"aclImdb/train\", batch_size=batch_size\n",
        ")\n",
        "val_ds = keras.utils.text_dataset_from_directory(\n",
        "    \"aclImdb/val\", batch_size=batch_size\n",
        ")\n",
        "test_ds = keras.utils.text_dataset_from_directory(\n",
        "    \"aclImdb/test\", batch_size=batch_size\n",
        ")\n",
        "train_ds_3 = keras.utils.text_dataset_from_directory(\n",
        "    \"aclImdb/train3\", batch_size=batch_size\n",
        ")\n",
        "\n",
        "text_only_train_ds_3 = train_ds_3.map(lambda x, y: x)\n",
        "text_vectorization.adapt(text_only_train_ds_3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4d5b3b17-dccd-42a7-d15f-ff132f382311",
        "id": "psWwqminzH9c"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 13902 files belonging to 2 classes.\n",
            "Found 10000 files belonging to 2 classes.\n",
            "Found 25000 files belonging to 2 classes.\n",
            "Found 998 files belonging to 2 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Establishing another model using an embedding layer training with 998 samples"
      ],
      "metadata": {
        "id": "lQqE-gLaRUnJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I'm still keeping the validation samples to 10,000, cutting off reviews after 150 words, and considering only the top 10,000 words for this model and the next model."
      ],
      "metadata": {
        "id": "0rU1B132Rjp2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras import layers\n",
        "\n",
        "max_length = 150\n",
        "max_tokens = 10000\n",
        "text_vectorization = layers.TextVectorization(\n",
        "    max_tokens=max_tokens,\n",
        "    output_mode=\"int\",\n",
        "    output_sequence_length=max_length,\n",
        ")\n",
        "text_vectorization.adapt(text_only_train_ds_3)\n",
        "\n",
        "int_train_ds = train_ds.map(\n",
        "    lambda x, y: (text_vectorization(x), y),\n",
        "    num_parallel_calls=4)\n",
        "int_val_ds = val_ds.map(\n",
        "    lambda x, y: (text_vectorization(x), y),\n",
        "    num_parallel_calls=4)\n",
        "int_test_ds = test_ds.map(\n",
        "    lambda x, y: (text_vectorization(x), y),\n",
        "    num_parallel_calls=4)\n",
        "int_train_ds_3 = train_ds_3.map(\n",
        "    lambda x, y: (text_vectorization(x), y),\n",
        "    num_parallel_calls=4)"
      ],
      "metadata": {
        "id": "axZtzsM6RgQQ"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_layer = layers.Embedding(input_dim=max_tokens, output_dim=256)"
      ],
      "metadata": {
        "id": "YhQ1VlwdSXX3"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "inputs = keras.Input(shape=(None,), dtype=\"int64\")\n",
        "embedded = layers.Embedding(\n",
        "    input_dim=max_tokens, output_dim=256, mask_zero=True)(inputs)\n",
        "x = layers.Bidirectional(layers.LSTM(32))(embedded)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
        "model = keras.Model(inputs, outputs)\n",
        "model.compile(optimizer=\"rmsprop\",\n",
        "              loss=\"binary_crossentropy\",\n",
        "              metrics=[\"accuracy\"])\n",
        "model.summary()\n",
        "\n",
        "callbacks = [\n",
        "    keras.callbacks.ModelCheckpoint(\"embeddings_bidir_gru_with_masking_2.keras\",\n",
        "                                    save_best_only=True)\n",
        "]\n",
        "model.fit(int_train_ds_3, validation_data=int_val_ds, epochs=10, callbacks=callbacks)\n",
        "model = keras.models.load_model(\"embeddings_bidir_gru_with_masking_2.keras\")\n",
        "print(f\"Test acc: {model.evaluate(int_test_ds)[1]:.3f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "afa40adc-450e-4121-ffb7-efa81be56759",
        "id": "TEHcSqfQSbKB"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_3\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_4 (InputLayer)        [(None, None)]            0         \n",
            "                                                                 \n",
            " embedding_4 (Embedding)     (None, None, 256)         2560000   \n",
            "                                                                 \n",
            " bidirectional_2 (Bidirecti  (None, 64)                73984     \n",
            " onal)                                                           \n",
            "                                                                 \n",
            " dropout_3 (Dropout)         (None, 64)                0         \n",
            "                                                                 \n",
            " dense_4 (Dense)             (None, 1)                 65        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2634049 (10.05 MB)\n",
            "Trainable params: 2634049 (10.05 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "Epoch 1/10\n",
            "32/32 [==============================] - 16s 236ms/step - loss: 0.6924 - accuracy: 0.5190 - val_loss: 0.6904 - val_accuracy: 0.5505\n",
            "Epoch 2/10\n",
            "32/32 [==============================] - 5s 151ms/step - loss: 0.6726 - accuracy: 0.6493 - val_loss: 0.8069 - val_accuracy: 0.5011\n",
            "Epoch 3/10\n",
            "32/32 [==============================] - 5s 145ms/step - loss: 0.5915 - accuracy: 0.7164 - val_loss: 0.6049 - val_accuracy: 0.6866\n",
            "Epoch 4/10\n",
            "32/32 [==============================] - 4s 136ms/step - loss: 0.4329 - accuracy: 0.8437 - val_loss: 0.6040 - val_accuracy: 0.6896\n",
            "Epoch 5/10\n",
            "32/32 [==============================] - 4s 126ms/step - loss: 0.2680 - accuracy: 0.9018 - val_loss: 0.7716 - val_accuracy: 0.5736\n",
            "Epoch 6/10\n",
            "32/32 [==============================] - 4s 112ms/step - loss: 0.1418 - accuracy: 0.9579 - val_loss: 0.5716 - val_accuracy: 0.7588\n",
            "Epoch 7/10\n",
            "32/32 [==============================] - 4s 113ms/step - loss: 0.0793 - accuracy: 0.9820 - val_loss: 0.6639 - val_accuracy: 0.7372\n",
            "Epoch 8/10\n",
            "32/32 [==============================] - 4s 115ms/step - loss: 0.0410 - accuracy: 0.9950 - val_loss: 0.7429 - val_accuracy: 0.7640\n",
            "Epoch 9/10\n",
            "32/32 [==============================] - 3s 111ms/step - loss: 0.0278 - accuracy: 0.9940 - val_loss: 0.6462 - val_accuracy: 0.7632\n",
            "Epoch 10/10\n",
            "32/32 [==============================] - 4s 110ms/step - loss: 0.0203 - accuracy: 0.9980 - val_loss: 0.8922 - val_accuracy: 0.7649\n",
            "782/782 [==============================] - 10s 8ms/step - loss: 0.5822 - accuracy: 0.7586\n",
            "Test acc: 0.759\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training a model with an embedding layer (trained from scratch) yields a validation accuracy of ~76.8% (at best) and a test accuracy of ~74.7% when using 998 training samples. This is a significant improvement in performance compared to only training on 100 samples.\n",
        "\n",
        "I'm now going to see how this performance compares to a model using a pretrained word embedding layer with 998 training samples."
      ],
      "metadata": {
        "id": "Cg9GX60oSpqx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Establishing another model using pretrained word embedding with 998 training samples"
      ],
      "metadata": {
        "id": "pVgMvNASTUdf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = keras.Input(shape=(None,), dtype=\"int64\")\n",
        "embedded = embedding_layer(inputs)\n",
        "x = layers.Bidirectional(layers.LSTM(32))(embedded)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
        "model = keras.Model(inputs, outputs)\n",
        "model.compile(optimizer=\"rmsprop\",\n",
        "              loss=\"binary_crossentropy\",\n",
        "              metrics=[\"accuracy\"])\n",
        "model.summary()\n",
        "\n",
        "callbacks = [\n",
        "    keras.callbacks.ModelCheckpoint(\"glove_embeddings_sequence_model_2.keras\",\n",
        "                                    save_best_only=True)\n",
        "]\n",
        "model.fit(int_train_ds_3, validation_data=int_val_ds, epochs=10, callbacks=callbacks)\n",
        "model = keras.models.load_model(\"glove_embeddings_sequence_model_2.keras\")\n",
        "print(f\"Test acc: {model.evaluate(int_test_ds)[1]:.3f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "05dd5a74-33d8-4e3c-faad-7862d0220aac",
        "id": "o8chyUD_Th5m"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_4\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_5 (InputLayer)        [(None, None)]            0         \n",
            "                                                                 \n",
            " embedding_3 (Embedding)     (None, None, 256)         2560000   \n",
            "                                                                 \n",
            " bidirectional_3 (Bidirecti  (None, 64)                73984     \n",
            " onal)                                                           \n",
            "                                                                 \n",
            " dropout_4 (Dropout)         (None, 64)                0         \n",
            "                                                                 \n",
            " dense_5 (Dense)             (None, 1)                 65        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2634049 (10.05 MB)\n",
            "Trainable params: 2634049 (10.05 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "Epoch 1/10\n",
            "32/32 [==============================] - 10s 174ms/step - loss: 0.6937 - accuracy: 0.5030 - val_loss: 0.6923 - val_accuracy: 0.5159\n",
            "Epoch 2/10\n",
            "32/32 [==============================] - 4s 126ms/step - loss: 0.6866 - accuracy: 0.5962 - val_loss: 0.6906 - val_accuracy: 0.5409\n",
            "Epoch 3/10\n",
            "32/32 [==============================] - 4s 125ms/step - loss: 0.6704 - accuracy: 0.6583 - val_loss: 0.6824 - val_accuracy: 0.5612\n",
            "Epoch 4/10\n",
            "32/32 [==============================] - 4s 127ms/step - loss: 0.6023 - accuracy: 0.7054 - val_loss: 0.6036 - val_accuracy: 0.6733\n",
            "Epoch 5/10\n",
            "32/32 [==============================] - 4s 112ms/step - loss: 0.4626 - accuracy: 0.7976 - val_loss: 1.1308 - val_accuracy: 0.5363\n",
            "Epoch 6/10\n",
            "32/32 [==============================] - 4s 121ms/step - loss: 0.3232 - accuracy: 0.8808 - val_loss: 0.5371 - val_accuracy: 0.7238\n",
            "Epoch 7/10\n",
            "32/32 [==============================] - 3s 103ms/step - loss: 0.1983 - accuracy: 0.9369 - val_loss: 0.5896 - val_accuracy: 0.7503\n",
            "Epoch 8/10\n",
            "32/32 [==============================] - 3s 101ms/step - loss: 0.1366 - accuracy: 0.9569 - val_loss: 0.5841 - val_accuracy: 0.7576\n",
            "Epoch 9/10\n",
            "32/32 [==============================] - 3s 100ms/step - loss: 0.0502 - accuracy: 0.9910 - val_loss: 1.1495 - val_accuracy: 0.6269\n",
            "Epoch 10/10\n",
            "32/32 [==============================] - 3s 90ms/step - loss: 0.0560 - accuracy: 0.9860 - val_loss: 0.8041 - val_accuracy: 0.7619\n",
            "782/782 [==============================] - 7s 7ms/step - loss: 0.5470 - accuracy: 0.7186\n",
            "Test acc: 0.719\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training a model with a pretrained word embedding layer yields a validation accuracy of ~75.6% (at best) and a test accuracy of ~73.8% when using 998 training samples. Again, this is a significant improvement in performance compared to only training on 100 samples.\n",
        "\n",
        "This model is still performing comparably to the model using an embedding layer trained from scratch, but appears to be performing worse overall when using 998 training samples (based on running both models multiple times at this training sample size). It appears it doesn't take a very large training sample size for the embedding layer model to outperform the pretrained word embedding layer model.\n",
        "\n",
        "I'm now going to decrease the training samples to about 500 to see if that is sufficient to allow the embedding layer model to outperform the pretrained embedding layer model."
      ],
      "metadata": {
        "id": "UCM4sarwUc8u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Decreasing the training samples to 486 (about 500)"
      ],
      "metadata": {
        "id": "UL-AYD7UWjMg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os, pathlib, shutil, random\n",
        "\n",
        "base_dir = pathlib.Path(\"aclImdb\")\n",
        "val_dir = base_dir / \"val\"\n",
        "train_dir = base_dir / \"train\"\n",
        "train_dir_4 = base_dir / \"train4\"\n",
        "\n",
        "for category in (\"neg\", \"pos\"):\n",
        "    os.makedirs(train_dir_4 / category)\n",
        "    files = os.listdir(train_dir / category)\n",
        "    random.Random(1337).shuffle(files)\n",
        "    num_train_samples_3 = int(0.035 * len(files))\n",
        "    train_files_3 = files[-num_train_samples_3:]\n",
        "    for fname in train_files_3:\n",
        "        shutil.move(train_dir / category / fname,\n",
        "                    train_dir_4 / category / fname)"
      ],
      "metadata": {
        "id": "m_nHNjvZWpvk"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow import keras\n",
        "batch_size = 32\n",
        "\n",
        "train_ds = keras.utils.text_dataset_from_directory(\n",
        "    \"aclImdb/train\", batch_size=batch_size\n",
        ")\n",
        "val_ds = keras.utils.text_dataset_from_directory(\n",
        "    \"aclImdb/val\", batch_size=batch_size\n",
        ")\n",
        "test_ds = keras.utils.text_dataset_from_directory(\n",
        "    \"aclImdb/test\", batch_size=batch_size\n",
        ")\n",
        "train_ds_4 = keras.utils.text_dataset_from_directory(\n",
        "    \"aclImdb/train4\", batch_size=batch_size\n",
        ")\n",
        "\n",
        "text_only_train_ds_4 = train_ds_4.map(lambda x, y: x)\n",
        "text_vectorization.adapt(text_only_train_ds_4)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5a808088-a30b-4d7b-8f55-ac142a51db67",
        "id": "3upWzb_PWsZy"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 13416 files belonging to 2 classes.\n",
            "Found 10000 files belonging to 2 classes.\n",
            "Found 25000 files belonging to 2 classes.\n",
            "Found 486 files belonging to 2 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Establishing another model using an embedding layer with around 500 training samples"
      ],
      "metadata": {
        "id": "oh-gBqxJYiUW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Again, I'm still keeping the validation samples to 10,000, cutting off reviews after 150 words, and considering only the top 10,000 words for this model and the next model."
      ],
      "metadata": {
        "id": "GoCVHx5NYnsF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras import layers\n",
        "\n",
        "max_length = 150\n",
        "max_tokens = 10000\n",
        "text_vectorization = layers.TextVectorization(\n",
        "    max_tokens=max_tokens,\n",
        "    output_mode=\"int\",\n",
        "    output_sequence_length=max_length,\n",
        ")\n",
        "text_vectorization.adapt(text_only_train_ds_4)\n",
        "\n",
        "int_train_ds = train_ds.map(\n",
        "    lambda x, y: (text_vectorization(x), y),\n",
        "    num_parallel_calls=4)\n",
        "int_val_ds = val_ds.map(\n",
        "    lambda x, y: (text_vectorization(x), y),\n",
        "    num_parallel_calls=4)\n",
        "int_test_ds = test_ds.map(\n",
        "    lambda x, y: (text_vectorization(x), y),\n",
        "    num_parallel_calls=4)\n",
        "int_train_ds_4 = train_ds_4.map(\n",
        "    lambda x, y: (text_vectorization(x), y),\n",
        "    num_parallel_calls=4)"
      ],
      "metadata": {
        "id": "UpXu6avWY8On"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_layer = layers.Embedding(input_dim=max_tokens, output_dim=256)"
      ],
      "metadata": {
        "id": "swZ4yTwDY_lj"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "inputs = keras.Input(shape=(None,), dtype=\"int64\")\n",
        "embedded = layers.Embedding(\n",
        "    input_dim=max_tokens, output_dim=256, mask_zero=True)(inputs)\n",
        "x = layers.Bidirectional(layers.LSTM(32))(embedded)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
        "model = keras.Model(inputs, outputs)\n",
        "model.compile(optimizer=\"rmsprop\",\n",
        "              loss=\"binary_crossentropy\",\n",
        "              metrics=[\"accuracy\"])\n",
        "model.summary()\n",
        "\n",
        "callbacks = [\n",
        "    keras.callbacks.ModelCheckpoint(\"embeddings_bidir_gru_with_masking_3.keras\",\n",
        "                                    save_best_only=True)\n",
        "]\n",
        "model.fit(int_train_ds_4, validation_data=int_val_ds, epochs=10, callbacks=callbacks)\n",
        "model = keras.models.load_model(\"embeddings_bidir_gru_with_masking_3.keras\")\n",
        "print(f\"Test acc: {model.evaluate(int_test_ds)[1]:.3f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cc004274-4f4d-4ed8-91ff-5b7d3d35224f",
        "id": "jej9wMcCZDTK"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_5\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_6 (InputLayer)        [(None, None)]            0         \n",
            "                                                                 \n",
            " embedding_6 (Embedding)     (None, None, 256)         2560000   \n",
            "                                                                 \n",
            " bidirectional_4 (Bidirecti  (None, 64)                73984     \n",
            " onal)                                                           \n",
            "                                                                 \n",
            " dropout_5 (Dropout)         (None, 64)                0         \n",
            "                                                                 \n",
            " dense_6 (Dense)             (None, 1)                 65        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2634049 (10.05 MB)\n",
            "Trainable params: 2634049 (10.05 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "Epoch 1/10\n",
            "16/16 [==============================] - 14s 410ms/step - loss: 0.6940 - accuracy: 0.4753 - val_loss: 0.6927 - val_accuracy: 0.5180\n",
            "Epoch 2/10\n",
            "16/16 [==============================] - 4s 246ms/step - loss: 0.6842 - accuracy: 0.6399 - val_loss: 0.6920 - val_accuracy: 0.5182\n",
            "Epoch 3/10\n",
            "16/16 [==============================] - 4s 254ms/step - loss: 0.6721 - accuracy: 0.7263 - val_loss: 0.6889 - val_accuracy: 0.5527\n",
            "Epoch 4/10\n",
            "16/16 [==============================] - 3s 219ms/step - loss: 0.6356 - accuracy: 0.8025 - val_loss: 0.6968 - val_accuracy: 0.5556\n",
            "Epoch 5/10\n",
            "16/16 [==============================] - 3s 223ms/step - loss: 0.5343 - accuracy: 0.7778 - val_loss: 0.6135 - val_accuracy: 0.6766\n",
            "Epoch 6/10\n",
            "16/16 [==============================] - 3s 204ms/step - loss: 0.3647 - accuracy: 0.8868 - val_loss: 0.8819 - val_accuracy: 0.6049\n",
            "Epoch 7/10\n",
            "16/16 [==============================] - 3s 224ms/step - loss: 0.2712 - accuracy: 0.9053 - val_loss: 0.7448 - val_accuracy: 0.6550\n",
            "Epoch 8/10\n",
            "16/16 [==============================] - 3s 212ms/step - loss: 0.1360 - accuracy: 0.9671 - val_loss: 0.5965 - val_accuracy: 0.7201\n",
            "Epoch 9/10\n",
            "16/16 [==============================] - 3s 198ms/step - loss: 0.0712 - accuracy: 0.9918 - val_loss: 0.6259 - val_accuracy: 0.7362\n",
            "Epoch 10/10\n",
            "16/16 [==============================] - 3s 219ms/step - loss: 0.0371 - accuracy: 0.9959 - val_loss: 0.7957 - val_accuracy: 0.6846\n",
            "782/782 [==============================] - 10s 7ms/step - loss: 0.5927 - accuracy: 0.7215\n",
            "Test acc: 0.722\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training a model with an embedding layer (trained from scratch) yields a validation accuracy of ~72.9% (at best) and a test accuracy of ~71.1% when using ~500 training samples. This is again a significant improvement in performance compared to only training on 100 samples, but not as good as training on close to 1,000 samples.\n",
        "\n",
        "I'm now going to see how this performance compares to a model using a pretrained word embedding layer with around 500 training samples to see if only ~500 training samples is sufficient to enable the embedding layer model trained from scratch to outperform the pretrained word embedding model."
      ],
      "metadata": {
        "id": "U3Je2iy9ZIXt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Establishing a model using pretrained word embedding with around 500 training samples"
      ],
      "metadata": {
        "id": "K7rodMSjZMMb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = keras.Input(shape=(None,), dtype=\"int64\")\n",
        "embedded = embedding_layer(inputs)\n",
        "x = layers.Bidirectional(layers.LSTM(32))(embedded)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
        "model = keras.Model(inputs, outputs)\n",
        "model.compile(optimizer=\"rmsprop\",\n",
        "              loss=\"binary_crossentropy\",\n",
        "              metrics=[\"accuracy\"])\n",
        "model.summary()\n",
        "\n",
        "callbacks = [\n",
        "    keras.callbacks.ModelCheckpoint(\"glove_embeddings_sequence_model_3.keras\",\n",
        "                                    save_best_only=True)\n",
        "]\n",
        "model.fit(int_train_ds_4, validation_data=int_val_ds, epochs=10, callbacks=callbacks)\n",
        "model = keras.models.load_model(\"glove_embeddings_sequence_model_3.keras\")\n",
        "print(f\"Test acc: {model.evaluate(int_test_ds)[1]:.3f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3b1f1100-6b78-4f97-a53a-940d292487f8",
        "id": "DMN9-sSHZYqb"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_6\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_7 (InputLayer)        [(None, None)]            0         \n",
            "                                                                 \n",
            " embedding_5 (Embedding)     (None, None, 256)         2560000   \n",
            "                                                                 \n",
            " bidirectional_5 (Bidirecti  (None, 64)                73984     \n",
            " onal)                                                           \n",
            "                                                                 \n",
            " dropout_6 (Dropout)         (None, 64)                0         \n",
            "                                                                 \n",
            " dense_7 (Dense)             (None, 1)                 65        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2634049 (10.05 MB)\n",
            "Trainable params: 2634049 (10.05 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "Epoch 1/10\n",
            "16/16 [==============================] - 7s 256ms/step - loss: 0.6934 - accuracy: 0.4938 - val_loss: 0.6936 - val_accuracy: 0.5270\n",
            "Epoch 2/10\n",
            "16/16 [==============================] - 3s 209ms/step - loss: 0.6813 - accuracy: 0.6235 - val_loss: 0.6918 - val_accuracy: 0.5348\n",
            "Epoch 3/10\n",
            "16/16 [==============================] - 3s 199ms/step - loss: 0.6530 - accuracy: 0.6770 - val_loss: 0.6466 - val_accuracy: 0.6407\n",
            "Epoch 4/10\n",
            "16/16 [==============================] - 3s 182ms/step - loss: 0.5829 - accuracy: 0.7325 - val_loss: 0.6682 - val_accuracy: 0.5780\n",
            "Epoch 5/10\n",
            "16/16 [==============================] - 3s 187ms/step - loss: 0.4592 - accuracy: 0.8354 - val_loss: 0.6046 - val_accuracy: 0.6794\n",
            "Epoch 6/10\n",
            "16/16 [==============================] - 3s 192ms/step - loss: 0.3665 - accuracy: 0.9177 - val_loss: 0.5924 - val_accuracy: 0.6935\n",
            "Epoch 7/10\n",
            "16/16 [==============================] - 3s 196ms/step - loss: 0.2617 - accuracy: 0.9527 - val_loss: 0.5894 - val_accuracy: 0.7080\n",
            "Epoch 8/10\n",
            "16/16 [==============================] - 3s 167ms/step - loss: 0.1667 - accuracy: 0.9774 - val_loss: 0.5925 - val_accuracy: 0.6898\n",
            "Epoch 9/10\n",
            "16/16 [==============================] - 3s 178ms/step - loss: 0.1509 - accuracy: 0.9753 - val_loss: 0.6268 - val_accuracy: 0.6787\n",
            "Epoch 10/10\n",
            "16/16 [==============================] - 3s 187ms/step - loss: 0.0969 - accuracy: 0.9877 - val_loss: 0.6355 - val_accuracy: 0.6546\n",
            "782/782 [==============================] - 6s 6ms/step - loss: 0.5889 - accuracy: 0.7047\n",
            "Test acc: 0.705\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training a model with a pretrained embedding layer yields a validation accuracy of ~71.4% (at best) and a test accuracy of ~68.8% when using ~500 training samples. This is again a significant improvement in performance compared to only training on 100 samples, but also not as good as training on close to 1,000 samples.\n",
        "\n",
        "It appears that the model with the embedding layer trained from scratch overall outperforms the model with the pretrained layer, even with only ~500 training samples. It appears that it doesn't take a large training sample for the model with the embedding layer trained from scratch to outperform the model with the pretrained embedding layer on this dataset."
      ],
      "metadata": {
        "id": "1NgEThoxC9ok"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Conclusion"
      ],
      "metadata": {
        "id": "2SXefdzWDCYN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas"
      ],
      "metadata": {
        "id": "2qOYKfqr-Ouh"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_models = [['Bag-of-Words', 100], ['Embed from Scratch', 100], ['Pretrain Embed', 100], ['Embed from Scratch', 998], ['Pretrain Embed', 998], ['Embed from Scratch', 486], ['Pretrain Embed', 486]]"
      ],
      "metadata": {
        "id": "IxEmcxNsVrmJ"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pandas.DataFrame(data_models, index=[1,2,3,4,5,6,7], columns=['Model Type', 'Train Sample Size'])\n",
        "print(df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b17ad20c-0711-427f-b575-d6751037e66e",
        "id": "HhhPVV99DkxN"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "           Model Type  Train Sample Size\n",
            "1        Bag-of-Words                100\n",
            "2  Embed from Scratch                100\n",
            "3      Pretrain Embed                100\n",
            "4  Embed from Scratch                998\n",
            "5      Pretrain Embed                998\n",
            "6  Embed from Scratch                486\n",
            "7      Pretrain Embed                486\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_avgs = [[71.2, 70.9], [53.0, 53.7], [54.0, 54.7], [76.8, 74.7], [75.6, 73.8], [72.9, 71.1], [71.4, 68.8]]"
      ],
      "metadata": {
        "id": "yKwuP-VIYCHB"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df2 = pandas.DataFrame(data_avgs, index=[1,2,3,4,5,6,7], columns=['Validation Accuracy', 'Test Accuracy'])\n",
        "print(df2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-ue92VINBVnR",
        "outputId": "3d39836d-62ec-4826-9abc-c84393c73b45"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   Validation Accuracy  Test Accuracy\n",
            "1                 71.2           70.9\n",
            "2                 53.0           53.7\n",
            "3                 54.0           54.7\n",
            "4                 76.8           74.7\n",
            "5                 75.6           73.8\n",
            "6                 72.9           71.1\n",
            "7                 71.4           68.8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plot = df2.plot.bar(ylim=(52.0, 78.0), rot= 0, title=\"Model Performance\")\n",
        "plot.set_xlabel(\"Model Number\")\n",
        "plot.set_ylabel(\"Accuracy (%)\")\n",
        "plot"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 489
        },
        "id": "VIzrd9x9Epw3",
        "outputId": "e226eee6-65a8-4881-d803-1ad199a1ff45"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<Axes: title={'center': 'Model Performance'}, xlabel='Model Number', ylabel='Accuracy (%)'>"
            ]
          },
          "metadata": {},
          "execution_count": 37
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## In conclusion, when using only 100 training samples, the model with a pretrained embedding layer performed very comparably to the model with an embedding layer trained from scratch, and at times, performed better than the embedding layer model trained from scratch. The model with a pretrained embedding layer yielded about 54.0% validation accuracy and 54.7% test accuracy compared to the embedding model trained from scratch which yielded about 53.0% validation accuracy 53.7% test accuracy.\n",
        "\n",
        "## When increasing the training sample size to 998, the model with an embedding layer trained from scratch outperformed the model with a pretrained embedding layer. The model with an embedding layer trained from scratch yielded a validation accuracy of ~76.8% and a test accuracy of ~74.7% compared to the model with a pretrained embedding layer which resulted in a validation accuracy of ~75.6% and a test accuracy of ~73.8%. Both of these models performed significantly better at 1,000 training samples vs. the initial 100 training samples.\n",
        "\n",
        "## After seeing that the model with an embedding layer trained from scratch outperformed the model with a pretrained embedding layer at around 1,000 training samples, I decreased the number of training samples to 486 (roughly half) to see if this was enough samples to allow the embedding layer trained from scratch to outperform the pretrained embedding layer. It appears that roughly 500 training samples is enough for the model with the embedding layer trained from scratch to perform better (validation accuracy of ~72.9%, test accuracy of ~71.1%). This is in comparison to the model with a pretrained embedding layer, which resulted in a validation accuracy of ~71.4% and a test accuracy of ~68.8%. It appears that it doesn't take a large training sample for the model with the embedding layer trained from scratch to outperform the model with the pretrained embedding layer on this dataset.\n",
        "\n",
        "## Lastly, the bag-of-word model displayed great potential, performing surprisingly well at only 100 training samples (71.2% validation accuracy, 70.9% test accuracy). Because this assignment was focused on models using embedding layers trained from scratch vs. models using pretrained embedding layers, I didn't attempt the bag-of-word approach on larger sample sizes, but I'm confident it would perform very well on a larger sample given how well it performed at only 100 training samples."
      ],
      "metadata": {
        "id": "TskhiFF0cbn8"
      }
    }
  ]
}