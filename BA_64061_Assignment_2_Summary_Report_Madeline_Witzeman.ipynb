{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "V100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# BA 64061 Assignment 2 Summary/Report"
      ],
      "metadata": {
        "id": "UeXlgNwZe465"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Madeline Witzeman"
      ],
      "metadata": {
        "id": "pq9Ue0v5l_xd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas"
      ],
      "metadata": {
        "id": "2qOYKfqr-Ouh"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_models = [['From Scratch', 1000, 'No', 'No', '70.0% -75.0%', '65.0% - 72.0%'], ['From Scratch', 1000, 'Yes', 'Yes', '80.0% - 85.0%', '79.0% - 83.0%'], ['From Scratch', 3000, 'No', 'No', '78.0% - 82.0%', '78.0% - 82.0%'], ['From Scratch', 3000, 'Yes', 'Yes', '90.0% - 92.0%', '89.0% - 92.0%'], ['From Scratch', 5000, 'Yes', 'Yes', '91.0% - 94.0%', '90.0% - 92.0%'], ['From Scratch', 2000, 'Yes', 'Yes', '87.0% - 91.0%', '90.0% - 92.0%'], ['Pretrain - Feature', 1000, 'Yes', 'No', '~98.0%', 'NA'], ['Pretrain - Feature', 1000, 'Yes', 'Yes', '~98.0%', '96.0% - 98.0%'], ['Pretrain - Feature', 3000, 'Yes', 'Yes', '97.0% - 98.5%', '97.0% - 99.0%'], ['Pretrain - Feature', 5000, 'Yes', 'Yes', '97.5% - 98.5%', '97.0% - 98.5%'], ['Pretrain - Feature', 2000, 'Yes', 'Yes', '97.0% - 98.0%', '97.0% - 98.5%'], ['Pretrain - Fine-tune', 3000, 'Yes', 'Yes', '97.5% - 98.3%', '97.0% - 98.0%']]"
      ],
      "metadata": {
        "id": "IxEmcxNsVrmJ"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pandas.DataFrame(data_models, index=[1,2,3,4,5,6,7,8,9,10,11,12], columns=['Network Type', 'Training Sample Size', 'Dropout Used?', 'Data Augment Used?', 'Validation Accuracy', 'Test Accuracy'])\n",
        "print(df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yKwuP-VIYCHB",
        "outputId": "761e3ae4-23ba-4cff-ce5c-c44abbb6eb4a"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "            Network Type  Training Sample Size Dropout Used?  \\\n",
            "1           From Scratch                  1000            No   \n",
            "2           From Scratch                  1000           Yes   \n",
            "3           From Scratch                  3000            No   \n",
            "4           From Scratch                  3000           Yes   \n",
            "5           From Scratch                  5000           Yes   \n",
            "6           From Scratch                  2000           Yes   \n",
            "7     Pretrain - Feature                  1000           Yes   \n",
            "8     Pretrain - Feature                  1000           Yes   \n",
            "9     Pretrain - Feature                  3000           Yes   \n",
            "10    Pretrain - Feature                  5000           Yes   \n",
            "11    Pretrain - Feature                  2000           Yes   \n",
            "12  Pretrain - Fine-tune                  3000           Yes   \n",
            "\n",
            "   Data Augment Used? Validation Accuracy  Test Accuracy  \n",
            "1                  No        70.0% -75.0%  65.0% - 72.0%  \n",
            "2                 Yes       80.0% - 85.0%  79.0% - 83.0%  \n",
            "3                  No       78.0% - 82.0%  78.0% - 82.0%  \n",
            "4                 Yes       90.0% - 92.0%  89.0% - 92.0%  \n",
            "5                 Yes       91.0% - 94.0%  90.0% - 92.0%  \n",
            "6                 Yes       87.0% - 91.0%  90.0% - 92.0%  \n",
            "7                  No              ~98.0%             NA  \n",
            "8                 Yes              ~98.0%  96.0% - 98.0%  \n",
            "9                 Yes       97.0% - 98.5%  97.0% - 99.0%  \n",
            "10                Yes       97.5% - 98.5%  97.0% - 98.5%  \n",
            "11                Yes       97.0% - 98.0%  97.0% - 98.5%  \n",
            "12                Yes       97.5% - 98.3%  97.0% - 98.0%  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "I began my analysis by starting with a simple convolutional network that I trained from scratch using 1,000 images for training, 500 for validation, and 500 for testing. In this initial network, I didn't utilize any dropout or data augmentation so I had an initial baseline to work with. The plots for the model indicated that the model was overfitting. Validation accuracy only hit ~70%-75% at most, and the validation loss increased as the number of epochs continued to grow beyond 10. Additionally, the test accuracy was only ~70%. To address the overfitting in the initial model, I employed data augmentation and dropout to see how it affected the validation and test accuracies. After including data augmentation and droupout in the initial model, the validation accuracy increased to ~80-85% and the test accuracy increased significantly to ~79-83%. Using data augmentation and dropout significantly improved model performance."
      ],
      "metadata": {
        "id": "7ZKdHP2Se_Rq"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "51649nBL3BXx"
      },
      "source": [
        "My next approach involved varying the training sample size while keeping the validation and test sample size constant. I first increased the training sample from 1,000 images to 3,000 images while using no dropout or data augmentation. The plots indicated that increasing the training sample size from 1,000 images to 3,000 images improved performance: validation accuracy hit around 78%-82%, and the test accuracy was ~78%-82%. However, there appeared to be overfitting again. I  wanted to see if I could improve model performance and reduce overfitting by employing data augmentation and dropout."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pU0_19fDvbeK"
      },
      "source": [
        "After both increasing the training sample size to 3,000 images and using data augmentation and dropout, model performance improved significantly: the validation accuracy was ~90%-92% and the test accuracy was ~89%-92%. Since the initial models for a training sample size of 1,000 images and 3,000 images were indicating overfitting, I decided to automatically utilize data augmentation and dropout moving forward when generating new models and assessing performance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZwQySX4A2r8J"
      },
      "source": [
        "My next appraoch involved increasing the training sample size again to 5,000 images, which slightly increased the validation accuracy to ~91%-94% and kept the test accuracy around ~90%-92%. However, there were some odd/concerning trends depicted on the training and validation accuracy and loss plots. Both training and validation accuracy started dropping after ~35 epochs. Having the training accuracy drop as the model continues to train is the opposite of what I'd expect. Based on my research, it seems this may be related to using the rmsprop optimizer or the training rate, but I'm not entirely sure. Additionally, the validation accuracy was actually above the training accuracy across several epochs. I'm also not sure the exact reason for this, but am wondering if the validation dataset may be too small compared to the larger training dataset and therefore this is something happening by chance. Lastly, it's interesting that both validation and test loss are low throughout training, with the exceptions of a few data points where the validation loss spikes. Given the odd trends displayed on the training and validation accuracy and loss plots, I had concerns utilizing this model.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "exiX72CpKxIp"
      },
      "source": [
        "Additionally, I also tried decreasing the training sample size to 2,000 images which resulted in a validation accuracy of ~87%-91% and a test accuracy of ~90%-92%. These are very similar results to the model with a training sample of 3,000 images (and data augmentation + dropout), but the model using 3,000 images for training appeared to be slightly better."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "After making several adjustments while training a network from scratch, I repeated the steps above using a pretrained network. I chose to utilize the VGG16 convolutional base since it was trained using animal pictures (several cats and dogs) from ImageNet. I started with a simple pretrained network that employed dropout but no data augmentation. Using a pretrained network with dropout (and no data augmentation) noticeably improved performance: the validation accuracy increased to ~98%. The test accuracy couldn't be computed yet because the shape of the test dataset wasn't the same as the pretrained network shape. I next decided to add data augmentation to the pretrained network to see how it would affect performance. Using a pretrained network with both dropout and data augmentation resulted in a validation accuracy of ~98% and a test accuracy of ~96%-98%. This was a significant improvement in performance over any of the networks trained from scratch. Given the increase in model performance, I chose to utlize dropout + data augmentation in the pretrained networks moving forward."
      ],
      "metadata": {
        "id": "ww4h6mO_iqgz"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vSOVees-03VD"
      },
      "source": [
        "Similar to the steps above used in training a network from scratch, I next increased the training sample size to 3,000 images and 5,000 images, and also decreased the training sample size to 2,000 images. Increasing the training sample size from 1,000 images to 3,000 images increased validation accuracy to ~97.0%-98.5% and test accuracy to ~97.0%-99.0%. This was the model with the best performance so far.\n",
        "\n",
        "Increasing the training sample size to 5,000 images kept validation accuracy around ~97.5%-98.5% and test accuracy to ~97.0%-98.5%. However, similar to when I trained the model from scratch using 5,000 images, there were strange trends displayed on the training and validation accuracy and loss plots: validation accuracy was consistently higher than training accuracy, yet both training and validation loss were very low. Given these strange trends, I believe increasing the training sample to 5,000 images (or more) isn't the best approach. Decreasing the training sample size to 2,000 images produced a validation accuracy of ~97.0%-98.0% and a test accuracy of ~97.0%-98.5%. This was overall slightly worse than the pretrained model that used a training sample size of 3,000 images."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BrtIdghm5tNc"
      },
      "source": [
        "Lastly, since I had been using feature extraction of pretrained models, I wanted to also see if taking the approach of fine-tuning a pretrained model improved performance. I chose to work with a training sample size of 3,000 images since that produced the best results when both training from scratch and using feature extraction of a pretrained model. Fine-tuning the pre-trained model using a training sample size of 3,000 images resulted in a validation accuracy of ~97.5%-98.3% and a test accuracy of ~97.0%-98.0%. The training and validation accuracy plot depicted somewhat erratic validation accuracy that frequently dropped and increased. Additionally, the training and validation loss plot also depicted a concerning trend: the validation loss was consistently above 1 and even spiked above 1.5. Given these patterns, fine-tuning the model didn't improve performance, although there are more parameters that would need to be adjusted before entirely ruling out the fine-tuning approach."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ziLFBYsO_LBQ"
      },
      "source": [
        "# Conclusion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G6qql3ZE_Mce"
      },
      "source": [
        "## In conclusion, the pretrained model using feature extraction and a training sample size of 3,000 images overall performed the best. It appears that increasing the training sample size, up to a certain point, increases the model performance, whether a network is being trained from scratch or a pretrained network is being utilized. The threshold seems to be 3,000 images when keeping the validation and test datasets at 500 images each. The models that used the smallest training sample size (1,000 images) had the worst performance, which implies that the training sample needs to be large enough in order to promote effective learning.\n",
        "\n",
        "## Additionally, utilizing a pretrained model improved model performance over training a model from scratch. This makes sense given the pretrained model I worked from was trained on a dataset of animal pictures that included several cat and dog pictures. Lastly, utilizing both dropout and data augmentation helped reduce overfitting and improved model performance across both networks trained from scratch and pretrained networks."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nbconvert"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7-5pyP3svTOB",
        "outputId": "25aa1876-98c5-4287-9980-6c2578feded2"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nbconvert in /usr/local/lib/python3.10/dist-packages (6.5.4)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from nbconvert) (4.9.4)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from nbconvert) (4.12.3)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.10/dist-packages (from nbconvert) (6.1.0)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.10/dist-packages (from nbconvert) (0.7.1)\n",
            "Requirement already satisfied: entrypoints>=0.2.2 in /usr/local/lib/python3.10/dist-packages (from nbconvert) (0.4)\n",
            "Requirement already satisfied: jinja2>=3.0 in /usr/local/lib/python3.10/dist-packages (from nbconvert) (3.1.3)\n",
            "Requirement already satisfied: jupyter-core>=4.7 in /usr/local/lib/python3.10/dist-packages (from nbconvert) (5.7.2)\n",
            "Requirement already satisfied: jupyterlab-pygments in /usr/local/lib/python3.10/dist-packages (from nbconvert) (0.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from nbconvert) (2.1.5)\n",
            "Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.10/dist-packages (from nbconvert) (0.8.4)\n",
            "Requirement already satisfied: nbclient>=0.5.0 in /usr/local/lib/python3.10/dist-packages (from nbconvert) (0.10.0)\n",
            "Requirement already satisfied: nbformat>=5.1 in /usr/local/lib/python3.10/dist-packages (from nbconvert) (5.10.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from nbconvert) (24.0)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from nbconvert) (1.5.1)\n",
            "Requirement already satisfied: pygments>=2.4.1 in /usr/local/lib/python3.10/dist-packages (from nbconvert) (2.16.1)\n",
            "Requirement already satisfied: tinycss2 in /usr/local/lib/python3.10/dist-packages (from nbconvert) (1.2.1)\n",
            "Requirement already satisfied: traitlets>=5.0 in /usr/local/lib/python3.10/dist-packages (from nbconvert) (5.7.1)\n",
            "Requirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.10/dist-packages (from jupyter-core>=4.7->nbconvert) (4.2.0)\n",
            "Requirement already satisfied: jupyter-client>=6.1.12 in /usr/local/lib/python3.10/dist-packages (from nbclient>=0.5.0->nbconvert) (6.1.12)\n",
            "Requirement already satisfied: fastjsonschema in /usr/local/lib/python3.10/dist-packages (from nbformat>=5.1->nbconvert) (2.19.1)\n",
            "Requirement already satisfied: jsonschema>=2.6 in /usr/local/lib/python3.10/dist-packages (from nbformat>=5.1->nbconvert) (4.19.2)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->nbconvert) (2.5)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from bleach->nbconvert) (1.16.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.10/dist-packages (from bleach->nbconvert) (0.5.1)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat>=5.1->nbconvert) (23.2.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat>=5.1->nbconvert) (2023.12.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat>=5.1->nbconvert) (0.34.0)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat>=5.1->nbconvert) (0.18.0)\n",
            "Requirement already satisfied: pyzmq>=13 in /usr/local/lib/python3.10/dist-packages (from jupyter-client>=6.1.12->nbclient>=0.5.0->nbconvert) (23.2.1)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.10/dist-packages (from jupyter-client>=6.1.12->nbclient>=0.5.0->nbconvert) (2.8.2)\n",
            "Requirement already satisfied: tornado>=4.1 in /usr/local/lib/python3.10/dist-packages (from jupyter-client>=6.1.12->nbclient>=0.5.0->nbconvert) (6.3.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!jupyter nbconvert --to html \"/content/drive/MyDrive/Colab Notebooks/BA 64061 Assignment 2 Summary Report Madeline Witzeman.ipynb\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hjJIgZz0zh2o",
        "outputId": "df114575-7f23-4d85-b0ac-86f0e458d148"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[NbConvertApp] Converting notebook /content/drive/MyDrive/Colab Notebooks/BA 64061 Assignment 2 Summary Report Madeline Witzeman.ipynb to html\n",
            "[NbConvertApp] Writing 606371 bytes to /content/drive/MyDrive/Colab Notebooks/BA 64061 Assignment 2 Summary Report Madeline Witzeman.html\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "5dsC_i_K2EY4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f4c89d17-65a6-4a62-fc27-ba98c1fc30c7"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    }
  ]
}